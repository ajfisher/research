slug,title,tags,text,predicted_tags,top5_tags,top5_probs,candidate_missing_from_top5,low_confidence_actual_tags,missing_tags,potential_misapplied_tags,has_tags,predicted_positive
2007-02-15-please-nokia-slap-me-again-no-really.md,Please nokia slap me again - no really,"['business', 'mobile', 'nokia']","Please nokia slap me again - no really
I love Nokia phones. In fact you could probably say that I have had a love
affair with Nokia devices for over the last 10 years ever since my first one. Typically they sum up everything that is important to me about technology -
that it must look great and must function well too. Nokia just always seem to
push my buttons when it comes to putting down my cash and getting a new phone.
Don't get me wrong, I've dabbled with the dark side of Ericsson and then Sony
Ericsson, I'd love to love a Motorola - really I would, but there is just
something about that Finnish company that every 12 months when it's time for an
upgrade makes me want to give them another chance. Over the last few years, incredibly though, their products have got worse... My last three phones have been a 6600, an 7710 and now an N73. Every one of these have had internet, bluetooth, big screens, cameras and all
the usual gubbins. Why then have they got progressively worse in terms of functions? My 6600 had terrible battery and would often crash. My 7710 had great battery,
would often crash and didn't support most web page content even though it was a
wide screen touchscreen that was supposed to have a fully featured web browser. My n73 is even worse, admittedly though the Carl Zeiss lense on the 3mp camera
is a piece of art and takes crystal clear photos. The web browser is much
improved thanks to the Mozilla engine and the 3G connection I have which
renders web pages at high speed. The battery life is awesome as well. The only
problem with this one is that I can't make or receive phone calls without it
crashing - literally to a black screen of death - about 80% of the time. Vodafone won't acknowledge a problem because Nokia won't so there's no sending
it back - I pity everyone who has tried and had their phone returned a couple
of weeks later saying ""there's no problem"". It isn't just me either check this
out in google:
[http://www.google.co.uk/search?hl=en&q=problems+nokia+N73+crash&meta](http://www.google.co.uk/search?hl=en&q=problems+nokia+N73+crash&meta)
and you'll see the extent of the problem. So taking a punt before xmas I bought an n770 internet tablet. Again a fine and
beautiful bit of kit, but again with it's fair share of bugs - not least a
*very* buggy version of opera which Nokia has decided not to support any more,
leaving it's internet tablet with a very bad internet web browser - ouch - 250
quid not well spent... well at least I can[ make a robot out of
it](http://www.pocketpicks.co.uk/latest/index.php/2007/01/17/meet-the-puppy-robot-with-a-nokia-770-tablet-for-a-head/). So I'm looking around again and I know that just like a jilted lover who
somehow thinks that ""things will be different this time"" I'll be back at
Nokia's bosum probably with an N95 in my hand... mmmm just look at the screen
and the slidey-outy-bit... 5mp camera, 3G with 11 GPRS slots for quick
downloading... I really don't need to make a phone call.","['business', 'mobile', 'nokia']","['mobile', 'business', 'nokia', 'web', 'development']","[0.6388046854473928, 0.6019687120344541, 0.5859533802903255, 0.31919495389719665, 0.3088934448970427]","['development', 'web']",[],[],[],True,3
2007-03-05-fuzzys-where-its-at-or-will-be-eventually.md,Fuzzy's where it's at... or will be eventually,"['algorithms', 'ecommerce', 'retail']","Fuzzy's where it's at... or will be eventually
I'm working on a project at the moment that took a remarkable turn recently.
Most clients we work on are fairly staid in their use of technology - which
suits our company as we are firm believers of the Keep It Simple Stupid
methodology of programming. I was in a meeting with a client who is a large retailer and we were talking
about ""filters"" for being able to reduce sets of data returned from the database.
Things like ""style"", ""size"", ""price"" etc - not dissimilar to
[Dabs](http://www.dabs.com) or any one of a thousand other online retailers.
Off the cuff I just said ""wouldn't it be good to use fuzzy logic on the filters
so instead of black and white result you get the shades of grey as well"". To be
honest I'm not even sure why I mentioned it... Imagine my surprise when the client said ""Show me""... Out came the pen and
paper and 15 minutes later he was sold on the idea and I was left to code an example. Fuzzy logic is a funny old beast - it is based around this notion that instead
of black and white you deal in shades of grey - black and white are just
extreme examples of the shades of gray. So <b>black might equal 0 and white might
be 1 but in between we can have 0.5 or even 0.3218956</b> if you so desire...
everything belongs to every group at least in part - even if that part is tiny,
or even 0. I love fuzzy logic - I played with it a lot at Uni when I was studying Neural
Networks - but it has never made it into mainstream web use - mostly because
it is so difficult to implement with a database unless you do a lot of extra
background work. Background work most clients won't pay for. It does make a big difference though - take this as an example: Say you have two products, Product A is £295 and Product B is £305. Now suppose you have a filter, or a search query that says ""Give me everything
less than £300"" Obviously Product A gets returned but Product B wont if you are using discrete
maths as it isn't 100% lower than £300. In the fuzzy view of the world though we can say ""Give me everything
approximately less than £300"". Now <b>depending on your exact specification of
what ""approximately"" means Product B may well be returned</b>. Indeed most people
prepared to spend £300 will probably spend £320 so we could say Product B has a
95% fit for this result. As we get closer to £320 the relevance gets less so it
is less likely to be returned. Ahh, I hear you say, I can do this by just pushing my filter up. Yes you can but
then £321 is left out altogether again. Maybe £321 is not as relevant as £305
but it is more or less as relevant as £320 on this scale. Fuzzy logic has made huge strides in engineering particularly with control
systems for things like washing machines (if a load is heavy use more water,
if light use less) and airconditioning units (if it is hot turn on harder than
if I am more or less where I need to be) but it has never caught on big time
on the web. I think that now that fundamental systems are starting to get in place
(e-commerce etc is nowhere near as difficult as it used to be) then we will
start seeing clients and programmers starting to use their brains a bit more
and looking at how we can deliver the best experience for our customers.","['algorithms', 'ecommerce', 'retail']","['algorithms', 'retail', 'ecommerce', 'web', 'development']","[0.5895715453266043, 0.5873653501030803, 0.5855298598824135, 0.3308451924520318, 0.314650914158283]","['development', 'web']",[],[],[],True,3
2007-04-23-why-is-css-such-a-painful-tool.md,Why is CSS such a painful tool?,"['css', 'development', 'standards']","Why is CSS such a painful tool?
Looking at the title above you'd be expecting to see a rant covering the lines of CSS is rubbish, it doesn't work properly and why can't we go back to the days of nested tables and lots of little shim images. I love CSS though, I love the fact that I don't need an editor to edit code any more [trying to do complex layouts in the past it was mandatory to use Dreamweaver to get any degree of speed], I adore that my code is more or less semantically correct and that it searches well. Being a techie and advising people on hosting I also love the fact that all that bandwidth isn't being wasted on buffer code in HTML to do trivial layouts that isn't then cached. We all know CSS has it's quirks, particularly in IE but the one thing that gets on my goat more than anything else with CSS is that it is a layout language developed by a bunch of people that aren't programmers or designers. They are wannabe web typographists who happen to have the time to come up with and enhance a spec that the rest of us are then forced to use. This gripe raises its head with me every once in a while but really badly today. This is because we're developing a site that is really thematic in terms of look and feel and the various sections really have their own colour coding. The overall behaviour is the same from one section to another in order to maintain consistency but things like backgrounds, titles and link colours shift to the new pallette. The really painful thing with this is the inevitable rehashing of styles down the page - particularly if it is a really stylised design with a lot of lines in it etc. You find yourself constantly typing the same border: 1px solid #123456 or color: #abcdef over and over again. Then when you are in a new section all these items have to be re-instantiated. What I would give for some variables - say something like this: Then in my CSS I could generate a selector and then apply the relevant style variable. I wouldn't even need any conditional logic to be happy just a quick and dirty token replacement function to make it work. Admittedly I could set this up by just making a CSS file a PHP file instead and returning text/css instead of text/html etc but what happens if I'm using ASP or I have to do something that is just quick and dirty HTML for a couple of pages... or what about the performance hit if my site is serving up 1000 page views a minute [or more]. As a CSS advocate I tell people that it is faster to dish pages with CSS as it is cached, but then this method will kill most caching strategies. We're now at the point where even if this was something that was deemed as being useful by the [W3C](http://www.w3c.org) powers that be we won't see it incorporated until at least the CSS 3.0 spec - which should be coming to a web browser near you some time about 2011/12 and probably won't have wide support until about 2015 - if [NASA have their way](http://www.nasa.gov/home/hqnews/2006/dec/HQ_06361_ESMD_Lunar_Architecture.html) they'll almost have landed a man on the moon again by then! So please, W3C - we are in this mire because of a lack of consulation of developers who are implementing the standards you set out. Perhaps getting a development edition of a browser that incorporates these standards at an early stage so they can be played with would be a good direction of resources for the next phase of growth...","['css', 'development', 'standards']","['development', 'css', 'standards', 'web', 'media']","[0.6368813260112796, 0.6034664805914554, 0.5996371042493344, 0.34097417836715227, 0.30559134411778427]","['media', 'web']",[],[],[],True,3
2007-04-24-the-things-we-take-for-granted.md,The things we take for granted,"['design', 'ux']","The things we take for granted
I had one of those amazing moments as a techie last night where you get so gobsmacked by something that you think about it even a day or so later. I was doing something on a mate's computer. Fair enough he isn't the most computer literate person in the world but he gets by - he's online and uses the web, email etc to get things done - he's even bought things online too. So he falls firmly in the ""average user"" category - certainly into the definition of the average use we use when we spec out systems and I'd say anyone that works at a software house or digital agency would look at Dean in the same way. So what gobsmacked me? The scroll wheel on his mouse... I was editing a large file for him and scrolling quickly up and down using either the scroll wheel to flick me around the document or doing that windows thing where you depress the button and flick the mouse and you start scanning the document up or down at various speeds. Dean was sitting there amazed and just said ""I had no idea you could do that"". Initially I thought he was talking about the editing in the config file I was hacking but it turns out he didn't even realise there was a little wheel in there - as far as he was concerned it was a place to rest his finger and was part of the design of the mouse. This got me thinking about all these things we as techies take for granted - not the big things like being able to get your email on your mobile phone or being able to mashup 10 different data sources to produce something new and novel or even build a website from scratch using nothing but a text editor. I'm talking about things like people not knowing their mice can do different things, that your phone probably can browse the web as well as make calls. My wife still can't even write a text message. I come across people even now who don't have internet connections and plenty who are still on dialup. As our industry grows, how does this moving ""average user"" impact on the people who are below average? Do we create a digital divide not necessarily based on the ""haves and have nots"" but on the basis of ""cans and cannots""? Seeing things like this has made me wonder again about people's perception and their interaction with their computing environment. I remember sitting in on a user testing session way back and seeing someone have one of those moments where they ""got"" the way the web worked - you could tell because she said ""That's why that text has a line under it - I always wondered about that"". Anyone who's got a relli who is a timid computer user knows that they don't do things with it because they are afraid they'll break it. Trying to get them to do anything outside their comfort zone is really difficult. With computers getting cheaper and cheaper and the next influx of users coming on board, we that build systems and interfaces must be sure to remember that the ""average user"" isn't getting smarter or more stupid - they will initially be less experienced and turning away these less experienced users may result in less traffic / sales whatever further down the line if someone else comes along that will hold their hand and guide them through the process. In 10-15 years this won't be an issue - but it is now.","['design', 'ux']","['ux', 'design', 'web', 'media', 'rant']","[0.5886016637843664, 0.5856018685262006, 0.32665845699947554, 0.3149572377380856, 0.309075511898087]","['media', 'rant', 'web']",[],[],[],True,2
2007-04-29-when-css-goes-bad.md,When CSS goes bad,"['css', 'development', 'standards']","When CSS goes bad
If you do a lot of CSS work you'll have seen particular bugs time after time and how to deal with them, however when things do go wrong [and they will trust me] finding bug related information can be a nightmare. For instance, today I was doing some work on a site and it has had a bug for a few days now in IE. The typical peekaboo bug - if you haven't seen it, it is typically an IE6 thing whereby you rollover a link or element set to have a :hover state and then as well as the effect you want [bgcolor changing for example] something else happens too. In my case I was rolling over a navigation item which then duly changed colour and then chopped off the page at the bottom of the screen. Even more interestingly it only did it when I had my secondary [nested] navigation up. If it was only the primary it didn't do it. More interesting again was that when I had the two navs up, mousing over the primary one caused the page to disappear and mousing over the secondary caused the page to reappear... argh. Needless to say with a bug and change list spanning a couple sheets of A4 this was just left for a while. A quick google for [peekaboo bug](http://www.google.co.uk/search?hl=en&q=peekaboo+bug&meta=) didn't net much that was useful. Same with [position is everything](http://www.positioniseverything.net) - and this is the problem - nomeclature of bugs and their effects can be so difficult to find in CSS as you have a mix of designers, researchers and techies all calling things different things. Typically my best bet has always been to try and describe the effect in as many different ways as possible on the basis of matching someone elses exact phrasing - as you can imagine this is like trying to hit a dart board on the moon from Earth with your eyes closed. Perhaps someone will sit down and come up with an accurate way of dealing with web browser bugs that classifies them much like we have with CERT [Computer Emergency Response Team] who [classify vulnerabilities](http://www.kb.cert.org/vuls/) in OSes and Software. This would make life a lot easier when a bug was found, especially when it is a variation on an existing one like the peekaboo one I had this morning. In the end the document which helped me out was this one - [http://www.satzansatz.de/cssd/onhavinglayout.html](http://www.satzansatz.de/cssd/onhavinglayout.html) although it did take some trial and error to work out which element needed to have it applied. It works fine now though and my bug / change list is down under a page which is even better....","['css', 'development', 'standards']","['development', 'css', 'standards', 'web', 'mobile']","[0.6413273964850046, 0.5958651295194097, 0.5948450417280965, 0.3425397476663008, 0.3037440092501155]","['mobile', 'web']",[],[],[],True,3
2007-04-30-super-computer-required-to-simulate-half-a-mouse-brain.md,Super computer required to simulate half a mouse brain,"['ai', 'supercomputers']","Super computer required to simulate half a mouse brain
Scientists have published that the've used the [IBM Blue Gene L supercomputer](http://www.llnl.gov/asc/computing_resources/bluegenel/bluegene_home.html) to simulate half of a typical mouse's brain. More accurately they've simulated about half the neurons and just over half the number of synaptic connections for 10 seconds - which because the simulation was running at about a tenth of normal speed showed about 1 second's worth of realtime information. You can read the whole story [here](http://news.bbc.co.uk/1/hi/technology/6600965.stm) Don't get me wrong, the guys at Nevada Uni have my utmost respect. I studied a lot about cognition and neural networks when I was at Uni, in fact I specialised in it with a degree in Computer Science and Psychology so I have a fair grasp of how hard this is to do. What gets me going though is the reasons behind doing it. As can be seen here, [the top Supercomputer](http://www.top500.org/lists/2006/11) in the world can be brought to its knees by modelling half a mouse brain for a very limited period of time. The reason for this is the sheer number of connections (synapses) that occur between neurons - a single neuron in a mouse can influence the behaviour of about 8,000 other neurons. It doesn't take long for the cascade to build up and your computations to start slowing down. What I find most interesting is that Blue Gene is designed to simulate molecular interactions particularly associated with the degradation of US Nuclear Weapons but it grinds to a halt with half a mouse brain. I must say that when I was playing with this over 10 years ago we were talking about ant or fruit fly brains which are merely hundreds of neurons in size and our computers were falling over. Given that baseline, the achievement these guys have made is incredible, although using the most powerful computer on the planet just shows you how far we are from modelling a human brain. Human brains typically have about a 100 billion neurons with many thousands of synapses. Rough estimates put the number of connections at about a quadrillion synapses which for those of you that like zeros looks like this: 1,000,000,000,000,000 Also to note was that when this is done typically one uses random assignation for where the synapses end up, it isn't a true model of how a brain works as there would be too much information to configure and would have to be done by hand. In these models the neurons are loaded into the system then randomly assigned a number of dendrites which randomly point to other neurons. You don't get real behaviour as in hearing and vision and the like but you do get a sense of how the flow of stimulus and response works. My own conclusion from my studies and keeping abreast of the topic since leaving formal education behind is that small neural networks specialised to a particular task are more likely to have results than large scale applications like this. Even mother nature adopted this process as you can see in evolutionary history that old structures are built upon by new, more specialised ones - you only need to look at a reptilian brain and compare it with our own, particularly the basal ganglia cluster to see the similarities in structure and function. In pulling these structures together you can then start achieving something that is greater than the sum of its parts.","['ai', 'supercomputers']","['ai', 'supercomputers', 'web', 'development', 'mobile']","[0.5943653256776652, 0.5883603680788058, 0.31736116588491925, 0.31385684097611766, 0.3069893954269575]","['development', 'mobile', 'web']",[],[],[],True,2
2007-05-03-drmed-for-life.md,DRMed for Life,"['drm', 'media', 'piracy', 'rant']","DRMed for Life
In the news recently has been the whole thing about not only the copy
protection on [HD-DVD and Blu-Ray disks being
cracked](http://www.infoworld.com/article/06/12/29/HNdrmhacked_1.html) but
people posting digg links with decryption keys in them. I can understand Digg's
position in removing said posts until the community kicked off and [they then
decided they'll go down with the ship ](http://blog.digg.com/?p=74)if they got
prosecuted. Hurrah for someone over there seeing sense. One part of my brain always goes ""Hooray for the hackers"" whenever we hear
stories about DRM being hacked in whatever guise it has been created. Another
part of my brain, probably the more rational side I guess, does kick in
afterwards and say that putting these things out in the wild will enable more
software / media piracy and will incur costs for the companies that produce it
which will make them either raise costs or step up counter-piracy methods. I
never get to the ""woe is me"" stage like most media company execs do as they are
truly multi-billion dollar organisations so it's hardly going to come out of
the mail boy's pay cheque and they are unlikely to go bust. What I do question properly though is the rationale that got us here in the
first place. Since the 60s with tape-to-tape reels starting to replace vinyl
records, music, film and software piracy has got bigger and bigger. What has
happened though is nothing short of an arms race. Consistent through this
entire arms race have been three key points: That me, or anyone else, once they have bought a product has the right to
play or use it for their own personal enjoyment whenever they see fit. This
is the argument that most consumers will use - I might buy a CD album but I
want to play it on my MP3 player. I might buy a DVD but I want to play it on
my Linux laptop as well as on my TV. Companies that produce consumable media assume that anyone that wants to
copy a product is inherently up to no good and they are now labelled as
pirates and are probably taking the music / film / software and selling it
in backstreet market stalls. The profligacy of piracy is directly related to the first two points and how
policed piracy is within the community. One can directly see that paid for knock off copies of movies and music is
completely against the law as you are selling someone's work and is tantamount
to counterfeiting. However, a framework for dealing with these people exists
within the law and we are starting to see this go down. The media companies will tell you that it's because of their anti-copy
protection, however in reality it is because of better policing and it being
viewed as a black market operation and it having been historically a move
away from ""hard crimes"" that has occurred over the last 20 years. This argument doesn't wash at all with consumers. Once I purchase a piece of
media it is mine to use how I want on whatever device I choose. The barriers that are being put up by the media companies in their zero
tolerance to consumers is assuring their position as the ""big bad ogre"" in all
of this. Were they to engage with the consumers who are most likely to want to
move content from one form to another they would probably be able to reach a
solution. Indeed were they to strip all DRM from their content altogether and then spend
the money on producing better content or else supporting better policing they
would probably turn a larger profit. In the words of Nixon, ""I am not a crook"" - but I do want to watch Spiderman 3
when it comes out possibly on my TV from my XBOX, on my Linux Laptop, Windows
Media centre and my PDA. At the moment I'll be lucky if one of those four work
so I probably won't buy it at all. _Update 2024-12-31: Minor edits to fix typos and broken links_","['drm', 'media', 'piracy', 'rant']","['media', 'rant', 'piracy', 'drm', 'web']","[0.6315940771367403, 0.6258936412077327, 0.5912361234300917, 0.591235238733706, 0.3132434560904941]",['web'],[],[],[],True,4
2007-07-03-is-180-good-value-for-wii-sports.md,Is £180 good value for Wii Sports?,"['consumer electronics', 'gaming', 'media']","Is £180 good value for Wii Sports?
I am definitely a [Nintendo](http://www.nintendo.com/) fan boy. I've had every Nintendo console released on the market plus so many Game & Watches it's not funny. Call me sentimental but Nintendo has been a part of and is one of the definers of my life. So obviously when the [Wii ](http://wii.nintendo.com/)came out there was no question I was going to get one. The new controllers are awesome and just show you what a massive difference can be made in [computer-human interaction](http://en.wikipedia.org/wiki/Human-computer_interaction) by adding a couple of extra components and removing some wires. But for some reason this time I didn't get one on launch or near to launch date. The reason? Because I didn't see any games I'd play. In the end it wasn't until June that I finally went and got one. The reason? My wife said she wanted to play Wii Sports! This obviously made it easier to add another console under the TV in the lounge room so in it went. Now if you haven't played it, Wii Sports ships with the console so in that regard it is free. In it there are a series of very well built games ostensibly to demonstrate the different ways the controller can be used. Tennis, Golf, Baseball, Ten Pin and Boxing all make an appearance. My other half and child love it. For my wife it is a change of all the things she hates about computer games (pushing buttons that bear no relation to the action on screen) and my child (who is nearly three) just likes swinging the controller and playing with mum and dad. But something inside of me just isn't loving my Wii. I walk into GAME and check out all the releases and there's nothing I want badly enough to part with £40 for. I got Zelda more because I thought I should rather than because I thought it was amazing and it is a fantastic piece of software engineering but it has the feel of a MMORPG grind about it so whilst I've nearly finished it now it has left me tepid in a way that Ocarina of Time never did. At present the Wii is the most sold ""next gen"" console and regular ""sold out"" signs at GAME down the road suggest they are still selling like hot cakes. Back to my original question though which was; is £180 good value for Wii Sports? Personally I'm more likely to play a bit of Wii Tennis when I get home than watch TV. In real terms my wife and I have played a good dozen hours of Wii Sports each (mostly together) otherwise we'd have gone to the cinema or something which would be £15 a ticket for 2 hours. That alone is about £180 by itself and when you pass the controller over to a neice or nephew or child of a friend who hasn't got one you do get that warm techie buzz about someone ""getting it"" for the first time and I think that is probably worth £180 any day of the week.","['consumer electronics', 'gaming', 'media']","['media', 'gaming', 'consumer electronics', 'web', 'mobile']","[0.6241689278457385, 0.5988290985283986, 0.5895219089448417, 0.3211199297145301, 0.31202650618900024]","['mobile', 'web']",[],[],[],True,3
2007-08-25-jquery-saves-the-day.md,JQuery saves the day?,"['css', 'javascript', 'web']","JQuery saves the day?
If you haven't come across it yet there is a javascript library called [JQuery](http://jquery.com/) which is being developed as an open source project, designed to give us better control over our web pages and the things we can do with them. Thankfully [John Resig, Karl Sedburg and the others](http://docs.jquery.com/About/Contributors) have steered slightly away from the profligacy of AJAX libraries doing the rounds at the moment and produced a library that actually deals with some of the problems you face as a web developer or a designer - namely things like clients saying ""I'd really like the first paragraph after each header to be blue instead of black"". Now before I get shot down in a burst of ""you can do that using classes in your p-tags"" I'll say this - I don't want to, I shouldn't have to and it makes for ugly and unmaintainable code. Doing this just [papers over the gaping holes left in CSS ](http://technologytreason.blogspot.com/2007/04/why-is-css-such-painful-tool.html)and makes your HTML even less semantic than it already is. This is where jQuery comes in. The biggest area of development in this library has been in developing ""content selectors"" similar to the [CSS selector specification](http://www.w3.org/TR/CSS21/selector.html). The brilliant thing about these selectors is that we don't have to wait until browsers with CSS 3 in them turn up before we can use them - thus saving us about 5-6 years of waiting time. I'm a fan of Javascript in small doses - I'm not a fan of large scale AJAX where it is pointless to be loading information that you can get on a click anyway, 99% of my clients want to ensure accessibility and often Javascript breaks that. On a UI where responsiveness is key then AJAX is 100% appropriate but for the majority of sites it's a gimmick. However in this context we have a javascript library that can add depth to the interface and add consistency and markers that could only be achieved by a lot of proprietary hacks. This benefits usability without sacrificing accessibility and portability. If JavaScript is switched off you lose nothing that wasn't there before anyway; if it is then you get a whole lot more texture to the site. Watch this space as I think there will be a lot of development on this library over the next 12 months.","['css', 'javascript', 'web']","['web', 'javascript', 'css', 'development', 'internet']","[0.6545906230560015, 0.6130660583243231, 0.6086177409637381, 0.3434137458318494, 0.30232445938034047]","['development', 'internet']",[],[],[],True,3
2007-10-14-jquery-slideshow.md,JQuery Slideshow,"['css', 'development', 'javascript', 'web']","JQuery Slideshow
It seems JQuery is definitely gaining some traction as a useful library - not least because of the development of the [ThickBox Gallery library ](http://jquery.com/demo/thickbox/)by [Cody Lindley](http://www.codylindley.com/) which is seeing huge amounts of use around the web at the moment as a means for displaying galleries for product or photos without being constrained by the page template you are building for and by maintaining the semantic integrity of the HTML you have put into the page. The last cool feature is that you don't have to use the dreaded pop up which brings into play the whole pop-up-blocker issues. It seems redundant to talk about the ThickBox stuff other than to say it's a great bit of kit and well worth checking out if you need gallery display functionality, I've got my own little bit of JQuery code to document here. This came about due to a client wanting a gallery then not wanting a gallery because they didn't want to maintain all the thumbnails etc and so it evolved into a ""slideshow"". They didn't want to use flash due to the cost, but they were already using JQuery for other parts of their site anyway. As such I decided to have a go with building a JQuery slideshow with the animation API. For this example I'm assuming some degree of javascript familiarity so I can get to the guts of the code. Obviously you'll need the [JQuery library ](http://jquery.com/)- I'm using the current 1.2.1 version that is compressed so it's a light download. Next up we need a page with an image in it with an an id called ""bigimage"". We also need some javascript to set up an array with the image names in it that we want to load so let's do that: We need to trap the moment the document becomes ready to work with so we set up the special document ready function: What this function does is set the opacity of the image to 0 (ie invisible) then we get a reference to it in standard javascript and finally attach an event to it which fires on the onLoad event for the image (more about this in a minute). The addEvent function is given below and is a worker function to add an event handler for a particular object. Why do we want to add an event for the onLoad of the image? The answer to this lies in how we want to do the animation. Potentially we could have hundreds of images in an array. This slideshow fades an image in, displays it for several seconds, fades out, loads the next image and starts again. By trapping the onLoad event of the image we can use this event to start the animation sequence which finished with an instruction to load the next image. Only once the image is fully loaded does the sequence begin again. So our Document Ready method sets up the onLoad event handler, anim() which is listed below: This function is called every time a new image has finished loading, bringing the image from 0 opacity to 100% over a 1500 msec interval. Next it holds the opacity at 100% for 5 seconds and finally fades out over 1.5 seconds after which is calls the function animNext(). animNext is a function that deals with determining the next image in the sequence (in my case, wrapping back to the start if we get to the end) and then displaying it purely by changing bigimage's SRC property. This is pretty straightforward JavaScript so I'll leave it for the reader to do. The key thing here is that by adding an event handler onto a low level object in the document along with a couple of animation commands a reasonable slideshow effect was created which works well for the users and was good for the client as it is maintainable and didn't cost a huge sum as it would have done in flash. It's the ability of JQuery to expose enough variety of basic features to allow you to do this very quickly and easily. I have no doubt that after 10 years of writing javascript that I'd be able to do this all by hand. The questions are ""Do I want to?"" and ""Is it good value for the client if I do?"" - my answer to both of these is ""not on your nelly"".","['css', 'development', 'javascript', 'web']","['web', 'development', 'javascript', 'css', 'internet']","[0.6679756199919895, 0.6415710235594007, 0.6185920235152003, 0.5917347740173672, 0.304680107931991]",['internet'],[],[],[],True,4
2007-11-02-fah-goes-number-1-but-we-could-do-better.md,FAH goes number 1 but we could do better,"['distributed computing', 'gaming', 'supercomputers']","FAH goes number 1 but we could do better
[Folding at home](http://folding.stanford.edu/) (FAH) has taken the [Guiness World Record ](http://www.guinnessworldrecords.com/)for being the most powerful distributed computing network with a top speed of over 1 petaflop - (a thousand trillion calculations per second). This is a remarkable achievement and shows the immense power that can be brought to bear by spare computing power used in a distributed network. The key here though is massive parallelism which means the various nodes in the network (your PC or PS3) are all doing different jobs at the same time and are at various points through these jobs. This is what made FAH and the old title holder Seti at Home (a search for extraterrestrial life) so scaleable. Individual computers on the network download work units from the central repository, process them individually and then resubmit them back to the central core for post processing. This is in contrast to say the Earth Simulator of Japan, a massive supercomputer capable of running huge simulations with ridiculous numbers of variables and calculations very quickly but where everything is interdependent. Likewise the ultimate aim of the BLUE project from IBM and the US Department of Energy is to be able to simulate all the forces and atoms of a nuclear explosion to simulate what's happening to USA's aging atomic weapons stockpile as they are no longer allowed to perform live tests. This doesn't take anything away from their achievement, however it does go to show just how much wasted processing capacity there is lying around on the network. The FAH project ramped up from 250 Teraflops (trillions of instructions per second) to just over a petaflop by the introduction of 670,000 PS3 owners supplying their hardware, up from the 200,000 PC users who got it to 250 Teraflops. Given that there are over 6 million PS3s in the wild this represents about 10% of the total Ps3 userbase - a quick calculation indicates that PS3 owners alone, should they all connect up to the internet, could provide about 7.5 Petaflops of processing power... this is beore we take into account PCs, XBoxes and Nintendo Wiis. What this illustrates to me is that many of these projects are limited by their publicity and how ""glamourous"" they are. Taking nothing away from the geekiness of searching for ET or the importance of seeing how protein folding will affect drug development in the future, a more elegent solution would be an open framework that users subscribe to which is then used by anyone who wants to create a distributed processing application. For the end user it is seamless and the for the multitude of public projects requiring raw processing cycles it gives them to opportunity to get access to larger numbers than their marketing budget would otherwise provide for. Even private companies could pay to rent processing time thus investing funds back into the project for ongoing development or optimisation.","['distributed computing', 'gaming', 'supercomputers']","['supercomputers', 'gaming', 'distributed computing', 'web', 'development']","[0.5897027088571746, 0.5876617597008439, 0.5817040732940799, 0.31549732277120424, 0.3104792826001498]","['development', 'web']",[],[],[],True,3
2007-11-08-why-cant-i-have-100-laptop.md,Why can't I have $100 laptop,"['government', 'hardware']","Why can't I have $100 laptop
Don't you hate it when you can't get something you'd really like? I've been following the [OLPC project](http://www.laptop.org/) more or less since its inception. When I first heard about it I was mostly interested in how they were going to pull off building a laptop for only $100 per unit. After realising they were going to do it I was interested in how useful the machine would actually be (it has no hard drive so it can't be that great right?). After seeing it was running Linux and was designed to be wireless from the start, run on mains or able to wind it up to power the laptop and it was designed to be durable in harsh environments I was mostly interested in how I could lay my hands on one (or two even). My disappointment was immense when the OLPC guys decided not to offer them for sale, and then when they u-turned and started the G1G1 initiative (Give One Get One) I had a momentary blip of joy until they said it would only be available in North America. Why they've not rolled this out to Europe is beyond my comprehesion - I don't even care if I don't have a £ key - I can always map it to a key stroke anyway. And I'd even be happy to Give 2 Get 1 if shipping was the issue. The other thing that amazes me is that given the connectivity of these laptops Western nations aren't falling over themselves to get them for schools - even if they had to pay a higher rate along the lines of the G1G1 programme it would still be cheaper than buying Dell machines into all the schools.","['government', 'hardware']","['hardware', 'government', 'development', 'web', 'internet']","[0.5882597565076078, 0.5874454718099367, 0.32042463140702127, 0.31746381521609934, 0.30026660860430954]","['development', 'internet', 'web']",[],[],[],True,2
2007-11-19-fuzzy-logic-could-book-more-flights.md,Fuzzy logic could book more flights,"['agents', 'algorithms', 'web']","Fuzzy logic could book more flights
I've talked about fuzzy logic for use by the retail sector [in the past](2007/03/fuzzys-where-its-at-or-will-be) and the project I'm involved in there is maturing nicely. This week I've really realised how, as software engineers we need to grasp the nettle and move a lot of service based software toward fuzzy systems for usability reasons. Nearly everyone these days has booked a flight online and when it came time to booking a holiday to Australia this winter, the first thing I did was fire up a browser and head to [expedia](http://www.expedia.co.uk/) and [travelocity](http://www.travelocity.co.uk/). If I was planning to fly on specific dates I would be well catered for and I could get a list of prices and book a flight in a few easy steps. I wasn't planning on flying on a specific date though. I work for myself so can take time off whenever I want in a general sense. Really what I wanted was the cheapest flight from London to Sydney in December. After typing a few different dates in manually I did the sensible thing and called a human travel agent who was very helpful. Unfortunately, as helpful as she was, she only had access to the same systems I did so couldn't tell me the info I needed to know. Mentioning this to friends had the usual ""you can't do that"" response. Can't do it?! I'm the customer I can book when I want. Most airlines operate through the SABRE booking network which is basically a massive database of flights from point to point with availability and prices per leg on it. It sits on top of a nice mature API which makes it easy to program against, and that's where the developers leave it. But as a customer this doesn't fulfill my requirements and this is where engineers need to spend more time thinking fuzzy. In these days of multi-processor and multi-threaded OSes it is not that difficult to build offline agents that could go and find this information out for a customer and then email it back to them. Indeed I wouldn't mind registering to use this sort of service so now the company has my personal details and they can market to me. The agent wouldn't even need to respond with all the availability. It could just give me the cheapest 10 or 20, all from a specific operator etc or those flights routing through Hong Kong as a stop over for example. It also doesn't need to be fast. A deprioritised thread could take a day to get this sort of information and if I'm being that vague then time is hardly an issue. If someone reads this from the travel industry please ask your techies to build this feature. If you are a venture capitalist then give me a call and we can revolutionise the online travel sector! The web has brought us an always on, on-demand, serviced-based method of interacting with our information but the casuality of this has been flexibility. The days of fuzzification are soon to be upon us and coupled with automated agents some amazing new systems will become available that will give us back our flexibility.","['agents', 'algorithms', 'web']","['web', 'algorithms', 'agents', 'development', 'business']","[0.6341126087588619, 0.5882863804243953, 0.5869326980393619, 0.3176456243914912, 0.30912299255736236]","['business', 'development']",[],[],[],True,3
2007-11-21-why-was-data-being-passed-on-a-disc-and-what-was-eds-advice.md,Why was data being passed on a disc and what was EDS' advice?,"['government', 'privacy', 'rant', 'security']","Why was data being passed on a disc and what was EDS' advice?
Readers in the UK will be aware of a Data Protection Act train crash that we have been watching unfold in front of us over the last few days. It turns out that 25 million records of a database managed by [HMRC](http://www.hmrc.gov.uk/) have been lost in the post because they were sent on a couple of disks using unrecorded mail. There has been much speculation about which minister to blame and who in the cabinet (including the Prime Minister) should lose their job but one thing that is mostly missing is the notion of data security. In the UK we have the Data Protection Act - policies enshrined in law to which I am constantly referring when talking to my clients. A typical day for me usually includes quoting something from the DPA at least once. Not least because a client wants to harvest user data and use it for something else that is outside the bounds of what is technically legal. I've done a lot of work for government and I have to say in my experience they have terrible technical practices. Gone are the days of locked down machines with no floppy drives and only CD-Rs. In are mass market units from Dell with the latest in CD/DVD-RW (because they are cheap and mass produced) along with USB connectors that people can hot plug a pen drive into and download whatever they like. The current government has a woeful record on technology projects mostly because they don't understand it and they contract suppliers who talk a good presentation rather than deliver an effective solution. According to the DPA ""Appropriate technical and organisational measures shall be taken against unauthorised or unlawful processing of personal data and against accidental loss or destruction of, or damage to, personal data."" This is why our PM said procedures weren't followed and he is bang on the money there. This relaxed attitude to data, particularly sensitive data, has been demonstrated in this debacle. If the data was going to be put on disc why wasn't it fully encrypted? Indeed, why wasn't there a secure online facility for user data to be interrogated without recourse to physical copies to begin with? In addition the data was supposed to have been ""desensitised"" before sending - a quaint term meaning removal of things like bank details, exact personal date and full address information. To do this EDS wanted to charge money for it. The department didn't want to pay so they took the lot. EDS are complicit in this as much as the people from HMRC are. How hard is it to type into the database ""Select name, age, postcode from person where...."" instead of ""Select * from person where..."" Or else just remove the columns that were sensitive on output. It would have taken me a few minutes so it can't have taken an experienced EDS engineer that long. EDS shouldn't have been charging for that sort of difference - but it sounds more complex so it was an opportunity to get some more cash in - probably. Further EDS should have been saying ""We advise you that the data you are requesting is excessive for the purposes of what you are going to use it for so we'll give you a more secure subset"". That would have rammed home the implications of what the staff at HMRC were asking for. In my history of working with government I have come across this sort of situation many times before. It is well known that government contractors over charge, shaking the fruit out of the infinitely laden money tree whenever they can. Our E-Minister is supposed to deal with this sort of thing but in practice he's a politician who knows as much about IT as my mum. The only way to resolve this problem is for wholesale changes to occur within government (locking down machines) and to make stiffer penalties the punishment for breaches of the DPA. We now have a situation where 25 million adults in the UK are worried that their personal details are going to be used in some sort of mass identity fraud. My view is pragmatic in that the CDs are propably laying in the corner of a sorting office at TNT somewhere - but they could well be in some gangster's tech lab being processed and that is the point of all this security.","['government', 'privacy', 'rant', 'security']","['rant', 'security', 'privacy', 'government', 'web']","[0.6238291722327777, 0.6099273847900156, 0.5909076127375172, 0.5853278345304938, 0.3201188941232141]",['web'],[],[],[],True,4
2007-11-26-adding-cron-jobs-to-a-qnap-server.md,Adding Cron Jobs to a QNAP server,"['development', 'devops', 'linux']","Adding Cron Jobs to a QNAP server
If you haven't come across them yet[ QNAP](http://www.qnap.co.uk/) make these amazing little NAS boxes that are perfect for home or SME use. I've got mine running as a home server but might get one for the office as our old server is on it's last legs and a fully tricked out 1U dell server is a bit of overkill for a glorified file server. The best thing about these devices though it that they run Linux OS utilising [Debian](http://www.debian.org/) Essential and as such they can be configured to do almost anything you want. Out of the box they already come with file serving, media serving, database and web servers. One slight problem though is that the boot up process is not disimilar to that of a live CD. This is great in that it makes the system highly robust and it boots to a known state each time. The problem is that short of rewriting the firmware you can't introduce things into the boot process. What I don't want to do is have to re-run a load of scripts to configure the server how I want it after a power failure or forced reboot. The boys over on the [QNAP forums](http://forum.qnap.com/) are really on the case and one of the chaps has created a nice little framework script which hooks into the boot process and allows the execution of a series of scripts. [You can see his work here](http://www.qnap.box.cx/). After installing this workaround you can add scripts to the scripts folder and take control of your server. One of the things I wanted to do was add items to my cron list and this process is explained below. SSH into your QNAP box Install the custom scripts files at [http://www.qnap.box.cx/](http://www.qnap.box.cx/) as per the directions there. CD to your scripts directory in custom and make a file called joblist.txt in VI (Vi is the only editor you have on the QNAP drive). When in vi make your list of cron jobs using the [standard CRON syntax](http://www.adminschoice.com/docs/crontab.htm). Mine was the following: This will run a backup script I had written at 1:25am everyday. You can add as many or as few as you want. Save your document and exit from Vi. Make your script that will fire on start up. I called mine cron_update.sh In there put the following code: Save and quite out of Vi. You'll notice I've used a variable in here to specify where to find the files. This is because the autorunmaster script is a folder higher so we need to be explicit about where to find things. Go back up a directory to your custom folder. In there edit your autorunmaster.sh file with vi. At the end of the file append: Then save and close the file. Now when you reboot you should have your newly added cron jobs appended to the crontab without removing all the old ones.","['development', 'devops', 'linux']","['development', 'linux', 'devops', 'web', 'internet']","[0.6418831178719757, 0.5913523579488147, 0.5822014906978034, 0.33332362576773533, 0.31222626479845733]","['internet', 'web']",[],[],[],True,3
2007-11-30-pci-dss-will-wreak-havoc-on-smes.md,PCI DSS will wreak havoc on SMEs,"['business', 'rant', 'security']","PCI DSS will wreak havoc on SMEs
One of my clients was asking me about [PCI DSS](http://www.pcisecuritystandards.org/) certification today. Coincidentally I also received our letter about compulsory compliance to the PCI DSS standard. Both of us are what are termed ""Level 4 Merchants"" - that is we process less than 20,000 card transactions through the company in a year. Arguably Level 4 Merchants will probably account for the largest number of business globally as they will incorporate pretty much every SME in PCI compliant countries that takes a card as a form of payment (according to Visa about 27 million businesses). The standard itself is a worthy document - a dozen set in stone compliancy rules to which businesses have to adhere. Most of it is common sense like settin your password on your router to something non-default, make sure card details are encrypted if they are to be stored, that sort of thing. Most businesses in the SME world would, in fact, actually be compliant - mostly because they don't store data. Here's the rub though. Barclaycard sent both my client and I a letter basically saying you have two options on compliance: First you do it yourself or otherwise you get someone to help you (and of course they recommend a company SecurityMetrics to help you do it all - at a discounted rate of course). Obviously the first thing I did was go to the security metrics site and request a quote. As a Level 4 Merchant it will cost me merely $699 per year to be assessed quarterly. However they can tell me do do things to get me up to spec which is then going to cost me more again. At the end of it they give me a pass or fail certification and their audit is completely subjective. After that I went and downloaded the whole specification and read it through twice. Every point I made a note against. Typically, this isn't a document for the feint of heart. I'm lucky first in that I'm a techie and second that I did my formative programming years in a bank specialising in what was then the forerunner of InfoSec. There is not a single line of ""plain english"" in the whole thing. A couple of non-techies I've shown it to got about a page in before giving up. Your average 1-5 employee company owner doesn't have a hope. Thus he'll end up paying $699 per year for what is essentially insurance. Even amongst Level 1 Merchants, understanding and compliance are two different things as you can see on [Evan Schuman's great article about recent stats to come out of the Level 1 camp](http://storefrontbacktalk.com/story/112907pciconfusion). Big companies have the resources to deal with this sort of stuff and they are also more likely to be saving data on customers so for them it is crucial. Whilst no less crucial for small businesses, the fact that a store owner who only takes card payments for people when they are physically in his shop will still have to go through this audit is patently ridiculous. BarclayCard are indemnifying themselves by playing the [FUD](http://www.google.co.uk/search?hl=en&q=define%3A+fud&meta=) card with comments like: To date these penalties have not been passed on to any Level 4 Merchants, but from 30th April 2008 your business will be liable for PCI DSS penalty charges and costs associated if you fail to comply or have a data compromise. Penalty charges can be considerable (in excess of £100,000) so, to protect your business, it is vital that your prepare for PCI DSS compliance by 30th April 2008 and continue to maintain compliance in the future. What the PCI DSS standard fails to deal with however is systematic failure of employee behaviour. It doesn't deal with issues such as people skimming cards if they are taken out of sight nor does it deal with employees writing details down on a piece of paper and passing them on when dealing with mail order, nor does it deal with phishing scams. Indeed I had a card machine problem last week and the support officer at BarclayCard stated: ""Just write the details down on a piece of paper and process them later"" Hardly a piece of advice that should be followed to maintain security. In the end businesses will have to make their own mind up about how to best deal with this new ""virtual legislation"" that is being thrust upon us. To me the whole thing reeks of the rise of the SEO industry piggybacking off Google's search technology. In reality the biggest source of credit card fraud is that caused by skimming details through offline processes such as mail order (which I had done to me recently and my bank caught it on the other end within a day) or else identity theft whereby a new card is created in someone else's name. None of the procedures outlined by the PCI DSS standard deal with these very real and growing issues - all they are doing are lining the pockets of consultant sharks that will feed on the SMEs who don't know any better and penalising the merchants for actually trying to conduct business.","['business', 'rant', 'security']","['rant', 'business', 'security', 'web', 'development']","[0.6174798133058984, 0.6078753475668849, 0.6020139005280974, 0.3221788663349476, 0.30529609131810637]","['development', 'web']",[],[],[],True,3
2007-12-10-net-xslt-and-how-to-import-an-external-xml-document.md,.NET / XSLT and how to import an external XML document,"['development', 'web']",".NET / XSLT and how to import an external XML document
I work with XML and XSLT every day of the week. Indeed working for a company called [XML Infinity ](http://www.xmlinfinity.com/)you can imagine how much we use it. I had one of those incredibly frustrating moments this afternoon that one typically when dealing with badly documented parts of .NET or XSLT. The annoyance in question was to do with loading a document in to an XSL template on the fly. 99.9% of the time you don't bother with this as you have a master XML document which you transform according to the XSL template that is assigned to it. All your XML processing is usually done before you get to this point. There is an xsl function though called document() which you can use to load in an external XML doc to the XSL template and then do work on it. I've used this before but the damn thing wouldn't work. Why not? Because our Transformation Engine wasn't using a loose enough resolver to be able to deal with externally referenced files... grrr. I know why MS did this because it's so the parsing engine doesn't go loading every document under the sun and potentially crashing. That's great but they could have documented it a bit better. The resolution by the way is to create an XmlUrlResolver, give it some credentials (in my case setting it to DefaultCredentials which allows you to access http::, file:: and https:: protocols) and then pass that into your Transform() method. Job done. Not quite. Having finally been given access to an external XML document I then had to contend with XSL's arcane methods of dealing with XML fragments. Again documentation was the issue here. Looking online there are some ridiculously complex ways of parsing an external document when by rights it should be as simple as just dropping the doc in a variable and then processing according to the variable. People were using recursive templates using xsl:copy and all kinds of things. Turns out the way to do it is a little known second parameter. If you do this: All you'll end up with is the text nodes. Not very useful. If you do this, however (note the second parameter): You'll end up with a full fledged XML document complete with nodes and everything put into your $var1 variable and you can then use it to select data according to standard XPATH constructs. If you don't want the whole document you can pass the second argument as an XPATH query and it will just return that nodeset - much easier to deal with. In all the time I've been dealing with XML / XSL I didn't know about this and it was a great pain to figure out. Typically the only reason I was doing this was to mock something up for a client quickly and it then turned into a mammoth effort. Knowing now though will save time subsequently I guess...","['development', 'web']","['development', 'web', 'rant', 'internet', 'media']","[0.642827673780956, 0.6421897043629399, 0.30298010590731494, 0.30235646323796866, 0.2968587737938131]","['internet', 'media', 'rant']",[],[],[],True,2
2007-12-19-sms-bamboozlement.md,SMS Bamboozlement...,"['mobile', 'rant', 'web']","SMS Bamboozlement...
I'm doing some work for a client at the moment who's industry is particularly technophobic. The absolute cutting edge is a bit of YouTube video thrown willy nilly into a page. I'd also point out that design is something that rarely makes an appearance in this particular industry. So it was pretty refreshing when we went to them with a series of ideas from the more commercial sectors of New Media and one of the things they latched onto was SMS. Queue annoyance though when we had already got everything ready to go other than to push the big green ""launch"" button and another company got involved and started talking about location aware services and high end data capture etc. At this point the client dissolved into a mess of indecision - ""Why weren't we doing all of this?"" was the question, to which the answer was ""Because you don't need to - primarily because your text messaging service is built around raising revenue through donations!"" I've had this happen in the past, notably with SEO companies. I do pity the poor clients who get stuck in these situations where they've finally decided to push their technology base along but then get waylaid by all the glittery, flashing and hypnotic LEDs. At the end of the day it is important to remember why you are doing something and not get sidetracked (and not get ripped off). Once a strong foundation of technology is laid there is always something new you can build - you don't have to have every shiny present under the tree to have a great christmas.","['mobile', 'rant', 'web']","['web', 'mobile', 'rant', 'development', 'media']","[0.6395884386862043, 0.6169885655533179, 0.6091719128960957, 0.3096233712252713, 0.3044070763834714]","['development', 'media']",[],[],[],True,3
2007-12-20-my-top-5-jquery-seasonal-wishes.md,My top 5 jQuery seasonal wishes,"['javascript', 'web']","My top 5 jQuery seasonal wishes
I've waxed lyrical about jQuery before, I've been using it a lot to do worker code which I just can't be bothered to hand write any more. Not least because jQuery handles all the little browser inconsistencies for me so the code I actually call into a page is infinitely more maintainable, especially if someone follows behind who maybe isn't so up to speed with JavaScript as I am. However, use a tool for long enough and closeness breeds contempt as they say. In this vein (and regular readers will know I don't do complimentary very often) and in the spirit of seasonal ""Listing programmes"" of every style, these would be the top 5 things I'd like to see incorporated into jQuery in the next year. Documentation - Starting off slowly and easily I'd definitely like to see some better documentation. Ideally I'd like to say that new sublibraries aren't included until their documentation is properly up to scratch. Some areas are very well documented other areas are sketchy at best. Wait(msecs, callback) - part of the effects sublibrary, we have all kinds of effects to enable objects to slide, fade and animate but we don't have a wait command. What I would give to have a command that you can just append to a sequence of animations and then wait for a period of time before calling another function or stepping to the next instruction. As you can see [from my jQuery Slideshow](http://technologytreason.blogspot.com/2007/10/jquery-slideshow.html) the common way to do this is to call animate() with the same instruction as your last step with a callback. It's not big or clever but it does the job. fadeToggle(speed) - again part of the effects sublibrary; we have slideToggle which is a great bit of code, call it and the object either slides open or shut depending on it's state. It would be great to have the same thing with fade rather than writing detection code and then calling fadeIn or fadeOut. State detection - Another worker function would be really useful here to actually determine the state of an object as to whether it is on or off in display terms. I am fully aware I can use document.getElementById(objname).style.display or equally $().css.display() however this will return ""none"" if it's off, but it could also return ""block inline table table-cell list"" etc depending on what it is. Ideally I'd like $().displayState() and it would return ""on or off"" or indeed true or false as a boolean so it would make display code even easier logic wise. And finally, Cast to DOM object. One of the best things about jQuery is it's query language. Using elements from the CSS and Xpath specifications pulling objects out of the document is so much easier than using DOM traversal methods. However sometimes the jQuery functions just aren't enough and we need to cast an object to real JavaScript to play with it - a simple method of doing this would mean the power of a great interrogation language along with the ability to cast to a real DOM object. I fully expect someone to come kick me now telling me I can do some or all of these things and indeed the functions I'm asking for exist already however the documentation as mentioned in number 5 is lacking in some areas so it isn't obvious if it is doable. Obviously this is a little tongue-in-cheek as if I was that worried about these issues I'd write the code myself and submit it to the team for inclusion in the next version. Indeed perhaps that could form the basis of one of my New Year's technology resolutions. Happy Holidays all.","['javascript', 'web']","['web', 'javascript', 'development', 'css', 'mobile']","[0.6533530636131162, 0.6121142410979199, 0.3406929096258714, 0.3038938517113617, 0.2972330227029071]","['css', 'development', 'mobile']",[],[],[],True,2
2008-01-08-the-warm-glow-of-site-launch.md,The warm glow of site launch,"['media', 'web']","The warm glow of site launch
I've been in this game a long time but there is still nothing sweeter than launching a site after spending a months building it with your team and the client. As a TD, site launch brings a mix of emotion - fatigue from the lack of sleep for the 10 days prior to launch, relief that the site is launching on time and on budget and the client seems happy with it all and finally worry about whether the thing will work as expected, what will everyone else think about it and by god I hope the server doesn't fall over on Day 1 under load... My grandfather was an engineer for Philips and he described to me the same feelings when they were launching a new product so I have a sense that irrespective of discipline, team based endeavours in engineering always foster the same heady mix of emotion fuelled by relief, adrenaline and fatigue. Whilst I am an old hand at this within this industry these days, having been here since the dawning, it is great to watch members of the team for whom this is the first of many site launches in their career and their happiness that it is done and their complete pride in their work. Having seen photos of workers completing railways and other major constructions in the 19th and early 20th century one can't help notice the parallels of young engineers completing a job regardless of whether they are working with steel, glass or lines of code.","['media', 'web']","['web', 'media', 'development', 'internet', 'mobile']","[0.6480472146004113, 0.6165715295391995, 0.31964987499468217, 0.30238902580180266, 0.3004843849905182]","['development', 'internet', 'mobile']",[],[],[],True,2
2008-01-31-the-state-of-oz-technology.md,The state of Oz technology,"['linux', 'mobile', 'security', 'web']","The state of Oz technology
Rarely does an entire country entice me to start ranting (and at this point I'll point out I am in fact Australian) but by crikey Australian technology hasn't really moved in the last 5 years. Now I appreciate this is a sweeping statement and I'll point out that the technology I'm talking about primarily is media based - mobile / web / internet. I have also had the benefit of living in London for the better part of 10 years so I've been at the hub of what is going on. What I don't understand is why is it that for a nation that was at the forefront of new media ten years ago are we now in a position where nothing has shifted for the last 5. SMS is still massively underutilised and the idea of an SMS shortcode in Australia is a joke - 8 digits is only 2 shorter than a mobile number so is hardly short! Indeed everything to do with mobile is still more expensive, slower and less polished than we are used to in Europe. I went to Vodafone when I got here and asked for a pay as you go sim card for my phone that had pay as you go data on it... I was met with blank stares - Telstra and Optus were both the same. General Internet access is similarly expensive and slow compared to what we are used to in Europe. Given a relatively modern telecommunications infrastructure, why telcos are flogging the ADSL route instead of fibre / cable begs the question of why so many roads were dug up in the capital cities to facilitate this in the late 80s and early 90s. What is also interesting is the lack of FOSS out here. Linux is relatively popular but no where like it is in Europe. Indeed corporate America has it's laser telescopic sight firmly trained on the Australian market and even getting Linux hosting is no where as simple as getting a site hosted on a windows server. Linux certification and knowledge is still seen as a specialist skill. Overall I'm disappointed that Australia hasn't maintained it's lead in Internet technologies. In part people like me are to blame for starting our careers here and then being drawn to the brighter lights of the UK and the US where visas are easily come by, pay levels are higher and the ability to work on cutting edge technologies are plentiful. Perhaps we are on the verge of a change in Australia and I hope that some of the ground lost can be regained over the next five years.","['linux', 'mobile', 'security', 'web']","['web', 'mobile', 'linux', 'security', 'development']","[0.6453154555528359, 0.6306672688883231, 0.5975164710596473, 0.5889153653047313, 0.30702606359902856]",['development'],[],[],[],True,4
2008-02-20-dvd-jon-strikes-again.md,DVD Jon strikes again,"['drm', 'media', 'piracy', 'rant']","DVD Jon strikes again
[DVD Jon or Jon Lech Johansen](http://en.wikipedia.org/wiki/Jon_Lech_Johansen)
as he is more commonly known gets a lot of love here. This great Norwegian
famously broke the DVD encryption put in place by the big firms with the
release of some software primarily aimed at allowing DVDs to be played on
computers and unlocking the regionality of DVDs and DVD players. When he released DeCSS he ran afoul of the [US
DMCA](http://en.wikipedia.org/wiki/DMCA) and was almost charged, he was then
indicted by Norwegian authorities acting on behalf of the US who actually did
go to court twice to try and convict him of hacking. Both times they failed and
decided not to go to the Supreme Court. Imagine our complete amusement in the office when we find out he's now trying
it on with Apple via iTunes. iTunes is a love it or hate it product - if you are part of the Apple / Steve
Jobs faithful it is obviously the greatest thing on earth, if you know nothing
about technology it's a simple product that allows you to use one of those
""fangled new digital music type thingies"". If you are a techie you see it as a proprietary lock in and try and avoid it
like the plague. The main issue for most techies is you can't play your music
on anything other than your PC / Mac that has iTunes installed and your iPod /
iPhone / iTouch. I've [railed against lock in](/2007/05/03/drmed-for-life/) for time immemorial -
just a quick count of my personal items puts the following music players at
my disposal - mobile phone (x2 because my wife has one that can play music
too), MP3 capable stereo, PC (x3 - my office, my home and laptop), PSP, Xbox, a
real MP3 player and my Nokia Internet Tablet - 10 devices at my personal
disposal that I want to play music from and indeed do play music from. The thing is, I know how to do all of this so I just push the files around on
memory cards or over my network (streaming from my media server for example)
onto the various devices. For many people this isn't possible and Apple's
enforcement of the iTunes lock ins firmly violate the right I have to play my
music (or video) on whatever device I choose at whatever time I choose. I also
vote with my wallet and don't buy tunes from Apple. What DVD Jon has done with his software (available from
[DoubleTwist](http://www.doubletwistventures.com/dt/Home/Index.dt) for free) is
allow you to take files that are locked into iTunes and essentially it plays
the file, re-encoding it into a format you can play on other devices (I haven't
looked properly but presumably OGG or MP3). Just to rub salt into the wound he's going to cause Apple and the US music
industry, he's decided to let you share your files with friends as well. One
wonders how long it will be before a writ arrive from the RIAA and Apple... I'm
sure they'll be racing to get in first. So well done Jon - keep up the good work and keep fighting the good fight -
media we have legitimately purchased is ours to use on any device we own for
our personal use. Eventually the media industry will wake up and realise where they've been going
wrong. Perhaps if EMI had taken notice of the way the world was going they
wouldn't have had to cull a couple of thousand staff.","['drm', 'media', 'piracy', 'rant']","['media', 'rant', 'piracy', 'drm', 'mobile']","[0.6268912877434504, 0.6124060052985133, 0.5917880344121862, 0.5917851912423934, 0.32336738235944906]",['mobile'],[],[],[],True,4
2008-03-15-security-101-the-user-should-be-able-to-authenticate.md,Security 101 : The user should be able to authenticate,"['rant', 'security', 'ux']","Security 101 : The user should be able to authenticate
Are you listening Barclays? I like security - particularly data security and in very particular data security that protects my personal information ([unlike a certain Uk government department a few months back](http://technologytreason.blogspot.com/2007/11/why-was-data-being-passed-on-disc-and.html)). However, I've been around this game long enough, worked for a [bank](http://www.citibank.com/) long enough and built more web applications capturing user data for long enough that I know there is one fundamental truth when it comes to data security and that is: **pragmatism**. When I was at Uni I was told, ""The only secure system is one that has no network connection, no keyboard or mouse and most of all no users"" (and I apologise [Dr Fekete](http://www.it.usyd.edu.au/about/people/staff/fekete.shtml) for bastardising your phrase but you can't have done a bad job for me to remember it 15 years later!). However the flip side of all of this was that depending on the data being protected, the security protocol should be appropriate without undue burden placed upon the user. Which is why logging into [flickr](http://www.flickr.com/) is trivial but logging into your bank should and is a more arduous affair. Banks are very secure enviroments which is good because the last thing I want is some 13 year old [script kiddie](http://en.wikipedia.org/wiki/Script_kiddie) making off with the tens of pounds in my bank account. Having said that, the bank should never make it difficult for me to get to the tens of pounds in my account due to security reasons. At the moment though banks are running very scared and they are nailing the customers because of it. On my recent trip to Australia I had my card stopped no less than three times because Barclays decided that the activity looked fradulent. Initially I thought something serious had happened but a call to Barclays got them to right the problem which was part of their new security measures. The next time it happened was because Barclays decided that it was time for me to come home and that I shouldn't be using my card in a Fraud Capital of the world like Sydney. The third time it happened though it locked my account out entirely and I was told I would have to come into a branch with identification documents to sort it all out - except there aren't any in Australia and I was leaving the next day for Hong Kong. Luckily a very understanding parent lent some cash. I applaud Barclays' sentiments - they really were trying to protect my account, however it would appear as though client / bank trust has disappeared and I can no longer say ""I want access to my money globally"" without alarm systems going off all over the place. If I was backpacking I'd have been in serious trouble as without a bailout I literally had about 10c in my pocket. Upon return to the UK Barclays' statement was along the lines of ""Sorry but we're dealing with a lot of fraud and it's better to be safe than sorry"". Tell this to one of my employees who just had £3K wiped out of their account due to identity theft (spent on local UK products and didn't fire off a single warning) and they are being told they have to prove it wasn't them... In a way I feel sorry for Barclays because they are damned one way or the other - on this issue though it should just be a case of phoning and doing a vocal authentication then saying ""I'm abroad for 4 weeks allow any transactions from xyz country until I say otherwise"". In this manner everything other than DDs occurring in my home country should be treated as fraudulent and everything authorised abroad should be fine... Bring on the chip in my hand is what I say...","['rant', 'security', 'ux']","['rant', 'security', 'ux', 'web', 'media']","[0.6189823209309034, 0.6082028373159978, 0.5859217567992037, 0.3289722349724434, 0.312402694924392]","['media', 'web']",[],[],[],True,3
2008-04-27-easy-product-or-class-rating-system.md,Easy product or class rating system,"['algorithms', 'development', 'internet', 'web']","Easy product or class rating system
So you've got a lovely little ratings system going on your site. All of a sudden
though you get slashdotted, dugg or just your marketing starts working and you
have thousands of users all rating your products / services / systems / posts /
videos etc and your pages start to creak. ""It's the shared web space you're on,"" say your techies, ""it can't handle the
users"" and duly bounce you to a better hosting environment at triple the cost
along with the migration charges. From time to time I come across this problem when I've either picked up code
from someone else or else a techie asks me how to optimise a page that's running
really slowly. In this particular instance it was caused by a ratings system in
the style of [Amazon](http://www.amazon.com/) or [YouTube](http://www.youtube.com/) -
basically a user is displayed a product and then people rate it as to whether
it's any good. The real problem came when they had a list of products, each of
which had it's individual ratings displayed. The cause of this very slow page however had nothing to do with shared hosting
or otherwise or direct server load - it was all down to some naive coding
executing what my old CS lecturer would call an O(n)2 process. What the coder had done was get a list of products, then for each product gone
back to the database and got a list of all the rankings ever made and then
averaged them out. Nice and simple but frightfully inefficient and that which
caused the problem I've highlighted. This isn't the first time I've seen this and I've been asked how to build them
numerous times as well so here's a well optimised method of doing it in general
terms. Consider first that calculating the average when you insert into the database
is going to be computationally less expensive than calculating it every time
you perform a select when a user hits the page. This sounds obvious but it's
stunning how often it's overlooked. Make two extra fields for your product table, one called average and the other
called user_count or something. On your insert of the rating into the ratings
table, run a trigger or else add some code that will update the product table
with the updated count and a new average calculated from the ratings info. Now when you select the product data you pull down the average and user count
as part of that select and they are just simple static fields, thus adding
no more computational load than the original select or view does already. This gives you a nice little rating system that's not heavy in terms of
processor load. However we can improve things once step further if you aren't
interested in the data. The option I'm providing below is good if you are just after a running average
and don't care about the individual ratings being kept. I did a project
recently where we weren't worried about keeping individual ratings data
because the site wasn't going to be up for very long and it didn't add anything
to our system to have it. This option uses a running weighted average in order to just update the data
in the product table without requiring a ratings table at all. Some useful background maths though: If I have a set {3, 4, 4} and take it's average I need to add the numbers and
divide by the number of entries. Thus this set's average is (3+4+4)/3 = 3.67 Now suppose I've precalculated this average as I've suggested above and stored
it without the individual ratings, I now want to add another rating, 2 to the set. Intuition says to do something like this: (2 + 3.67)/2 = 2.83 which is actually
wrong. Looking at the set {3, 4, 4, 2} we can guestimate that the average is
going to be somewhere more between 3 and 4 than it is 2 and 3 as we've
calculated above. Thankfully a technique from statistics gives us an option here which is to use
a weighted average instead. This is useful for adding sets together that have
different numbers of elements within them but maintain the averages by skewing
the data using proportional averages (or a weighted average). The general formula for this is: Where: `Avg1` is the average of the first set `Avg2` is the average of the second set `n1` is the number of elements in the first set `n2` is the number of elements in the second set In our example this simplifies even further because our second set is actually
only one item. So let's work this through: Which is the answer we're after for our average. As we know all the base line average data in the product table and we know the
value of the rating we're tracking, it's a very simple function to update this
instead of doing another insert into a ratings table and we just keep on doing
it for every rating that has been added. Computationally this is a very inexpensive process and whilst I'm more than
happy to be shown otherwise I think this is about as good as it gets in terms
of optimisation. The key thing is we've now reduced an O(n)2 operation to O(n) which is a
drastic improvement as n gets larger.","['algorithms', 'development', 'internet', 'web']","['web', 'development', 'internet', 'algorithms', 'media']","[0.6354127229567518, 0.6352219248075409, 0.6129446838707527, 0.5793686510471369, 0.31021896526975307]",['media'],[],[],[],True,4
2008-05-02-ubuntu-8-04-truly-desktop-linux.md,Ubuntu 8.04 - truly desktop Linux,"['desktop', 'linux', 'os']","Ubuntu 8.04 - truly desktop Linux
I'm quite an [Ubuntu](http://www.ubuntulinux.com/) fan, having followed the project since more or less it's original inception. Given the general lack of problems with it thus far you'll notice very few entries on this blog about it. Indeed various clients of mine are running Ubuntu servers that are easily maintained, easily managed and just generally easy and have been for several years. It's not necessarily an industrial strength OS - [see Fedora for example](http://fedoraproject.org/) - but for quick deployment, great security and stability and a modifiable tool (thanks to its Debian base) that just gets the job done you don't really need to look much further. But I think that's about to change. You see Ubuntu 8.04 (the latest version that also happens to be a Long Term Support version) has markedly shifted the goalposts of what I expect from a Linux distribution. I've been wanting to be convinced to move to a complete Linux desktop for the better part of 10 years but there's always something holding it back - lack of support for a media type, lack of drivers for particular bit of hardware, issues to do with wireless, no power management for my laptop, I can't run some third party apps like Skype... but that has all now changed - and changed in a massive way. As I'm want to do, every time a new version of Ubuntu comes out it is duly installed on my Acer Travelmate notebook - a very good test of whether an upstart OS ""just works"" or not. The machine is about 2 years old but it has some quirks such as it has an ATI 3D card embedded on it's motherboard that was difficult to get working properly even on XP, it also has an inbuilt webcam - again with proprietary Acer drivers and it has gigabit Ethernet. Oh and it's widescreen. About 9 times in 10 I don't even get to a working desktop without some hackery of graphics drivers, x.org files and I've even had a couple of ""bomb-proof"" distros just not even boot up to a command line. I've been around Linux for a long time and I know what to expect, I'm an enthusiast and advocate so none of this surprises me at all and I'm prepared to work through the issuses. Most of the time I get to a working desktop with some sort of graphical interface that is mostly not widescreen, with no 3d support, sometimes wireless and without the use of the webcam. Linux isn't aimed at desktop use - it's just a side effect of people using it for development who wanted some creature comforts whilst working - notably the [Gnome](http://www.gnome.org/) and [KDE](http://www.kde.org/) bods. Imagine my surprise when I booted Ubuntu 8.04 and I logged into a graphical desktop that detected wireless and gigabit ethernet, properly displayed my screen in widescreen mode, gave me the option to run my ATI drivers easily and then configured the 3D in a few seconds and on top of that gave me a working webcam that I'd never had running under Linux and you could tell from the whoops of joy that here was something worth formatting my hard drive for. Every device I threw at it was auto detected and installed in moments, flash drives, USB devices, a weather station, even an old MP3 player than needed proprietary Sony software to synch on XP. All handled with aplomb and with scarcely a pause by the processor. What the hell was going on? How did we go from solid and okay 7.10 to this awe inspiring 8.04 in just six months? Had Mark Shuttleworth finally given his soul to Beelzebub in exchange for the most promising distro to date? Then the answer came to me in a word: Vista. Vista - that problematic and misbegotten child of Redmond that has been causing havoc in the IT world for nearly a year now. I haven't installed it on a work machine, neither has any other techie I know that wants to ""Get Things Done"". My dad had it and tried it daily for 6 months - he's now back on XP. I know corporate users who've had it on new machines and reverted to XP in order to decrease the amount of support required for users. Shuttleworth and his cabal of Elite Ubuntu coders have recognised a change is in the air - particularly in Europe that is Linux's stronghold - we have an opportunity to put Linux on the desktop of millions of users who might upgrade to Vista but are worried about its impact. Couple this with a slight economic downturn and people are worried their existing hardware just won't work with Vista thus leading to a higher upgrade cost. This latest LTS version gives novice and power users alike the ability to do anything they want with their desktop and it just works. It gives corporate users the knowledge that they have the security of support for 5 years without the rug being pulled out from under them. We bit the bullet this week and put all our support team onto this version exclusively - WinXP was nuked off their machines. The development team are all dual booting but the number of XP desktops seem to be fading from view at a very fast rate as the requirement to just ""drop in"" on Windows becomes less necessary. We've been saying it for nearly a decade but ""this year is the year for desktop Linux"" and with 8.04 Ubuntu the excuses for moving OS can now be left at the door thanks.","['desktop', 'linux', 'os']","['linux', 'os', 'desktop', 'web', 'internet']","[0.6097529657670965, 0.6093293698010883, 0.5899595434339834, 0.32755083145944824, 0.3167567454773108]","['internet', 'web']",[],[],[],True,3
2008-05-09-eeepcs-power-is-in-the-network-not-the-machine.md,EEEPC's power is in the network not the machine,"['internet', 'linux', 'mobile', 'os']","EEEPC's power is in the network not the machine
It's official - I definitely am in love with [ASUS' EEEPC](http://eeepc.asus.com/global/). I liked the idea when they first came out and specifically trawled around Hong Kong computer markets to find one not long after they were launched. It's not the Apple Air or iPhone kind of aesthetic lust, I'm talking about true ""in sickness and in health"" type love when it comes to the EEEPC. Indeed for someone to now take this device off me it really would have to be from my cold, dead, rigamortis-set fingers - and then only with a saw. Go online and look at reviews. They fall into two camps - those who think it's great as a second machine that just happens to do a lot of funky things (see latest Linux Format June edition for a classic example) or those that just don't ""get it"" and wonder why the hell anyone would want a tiny-weenie machine when you can get a low spec dell for a few hundred quid now. There is also a third camp - who are starting to realise that a linux based UMPC is a truly brilliant bit of kit and it's because of the network it sits on not the thing plugged into it. I've had mine for about 5 months and realistically I've installed about half a dozen bits of software - 10 at a push. I can do docs, review spreadsheets, skype, web browse - hell even play games if I want and when hooked to a network I can do all of these things with all of the files I could possibly want. My machine comes home and it auto connects to my home network, syncs to my media server and can play all my media files out of the box. I can check my mail and actually read it without squinting without firing up the laptop. I can connect from home to work via a VPN and mod some files for a client without leaving the sofa or the garden and be doing what is needed before the laptop has finished booting to a desktop. At work I can use it for presentations and taking notes on projects without printing stupid amounts of documentation and hefting my laptop along with me. It's not a replacement computer - it's a tool -  <b>a finely shaped, infinitely configurable tool</b>. All the things I want in my phone but will never get because of the lack of keyboard, mouse and processing power and without it being much bigger. The thing is I'm a techie, if I'm talking a walk down the street phone and wallet are it. If I'm going somewhere then it's satchel with camera, book, PSP and now EEEPC in place of a laptop. ASUS have released details recently of a new version designed to hit off the people who think the EEEPC is too small. I don't know myself. Small is beautiful and in this case perfectly formed.","['internet', 'linux', 'mobile', 'os']","['mobile', 'internet', 'os', 'linux', 'web']","[0.6207401960282234, 0.6111211989622636, 0.605024358091539, 0.6016989321655517, 0.31581652763247553]",['web'],[],[],[],True,4
2009-07-21-the-golden-age-of-mobile-soon-maybe.md,The Golden Age of mobile? Soon maybe...,"['mobile', 'predictions', 'web']","The Golden Age of mobile? Soon maybe...
Some would say that it's already been - during the heady days of GSM Data and WAP, some would say it stalled when European clients pulled all funding from mobile internet apps in the post-dot-com-crash GPRS days, some would say that with the advent of the iPhone we're there in all it's shiny-coverflow-enabled-finger-waggling-goodness. It seems like every second person is now weilding some kind of internet enabled device and in Europe and the US the penetration is even higher than Oz although we are racing for a frontline position showing that reasonable access is more important than either coverage or cost. Half way through 2009 it's interesting to look at some of the predictions for this year - particularly where mobile is concerned and take a quick stock. The big 3 (Apple, MS, RIM) of last year are now well and truly the big 6 with highly competitive offerings from Android who we all knew had aspirations, the re-emergence of Palm with a life-recharging elixr known as Pre and of course Nokia firmly touting its Maemo platform that's been in development for many years and is arguably the most stable and feature rich of all. Costs for data access across the globe are plummeting with Vodafone in the UK offering the first truly unlimited data packages on phones, showing we live in a commodity market that is almost free. EU laws limiting the charges for call and data roaming will see uptake rise as people start using their phones across countries as well. Applications obviously make up a huge part of what our mobile experiences are like and I think if anything 2009 will go down in history as the year of the widget or micro app. Whilst iPhone still only supports Objective-C and Cocoa and their iron control is starting to hinder their progress on this front there are enough people keen to try and make a buck that the ecosystem around applications is phenomenal with over 50,000 available at last count. Nokia, MS, RIM and Palm all have app stores however these are fledgling compared to Apple's and of all the other players Android is the only one that can be considered a contender with approaching 20,000 apps available, the vast majority of which are free. Android has a very hands off approach to this so its interesting to see what makes it through compared to Apple's more militant approach. Being Java based is also helping Android be the largest growing development community too as it's super quick to get up and running. So where will we be in another 6 months? Will we look back and think 2009 is where it all started? I think it's a little premature. We are really at the start right now. Much of what we are doing on phones right now isn't much more than we were doing 5-6 years ago just with a bigger screen and prettier graphics. My money's on 2010 when we see a real rise of Augmented reality applications hit the phones. This is the area that will truly show what carrying the entire Internet around in your pocket can do and has been the spur for this part of computer science this year where it had languished for over a decade. When my phone can alert me when my friends are nearby, interact with environmental sensors, buzz me when a store within 500m is having a sale on an item I'd previously shown interest in, automatically adjust its settings dependent on where I am and the privacy level I want to adopt and filter all of the information on the Internet into a 3 inch screen in a way that is contextual and meaningful then I think we'll consider the Golden Age has started.","['mobile', 'predictions', 'web']","['mobile', 'web', 'predictions', 'media', 'business']","[0.6585547121098221, 0.6324553683508363, 0.5971372857318237, 0.31552437136023703, 0.3134067588507368]","['business', 'media']",[],[],[],True,3
2009-10-09-web-directions-south-2009-cloud-sourcing-the-business.md,Web Directions South 2009 - Cloud Sourcing the Business,"['business', 'cloud computing', 'conference', 'presentation']","Web Directions South 2009 - Cloud Sourcing the Business
I recently gave a presentation at Web Directions South which was fantastic (the conference I'm referring to here!). The session was on cloud computing and I hope everyone got something out of it. [I've put the presentation itself over at slideshare](http://www.slideshare.net/andrewjfisher/cloud-presentation-31odp) My speaking notes are below as the presentation won't make much sense without it as it's mostly images or single statements. Introduction Cloud computing is a fast moving part of the IT landscape right now and, media hype aside, it does actually represent a fundamental shift in the way we produce and consume IT services. Cloud computing simplified Partly because of where Cloud Computing resides on the hype curve but also because of it's pace of change, understanding the landscape can sometimes be confusing and it's probably best explained by this slide. Today I'm going to talk about what cloud computing actually is in practice, how cloud sourcing can be used in your business as well as it's future direction and some of the traps you need to be aware of. What is cloud computing? Cloud computing encompasses a huge array of services, technologies and techniques. Marketers would have you believe everything plugged into the Internet is a cloud. To understand the distinct areas of cloud computing let's try a little experiment. Hands up if you are right now using some kind of Cloud Computing Service in your business? Okay so if you are using some kind of infrastructure service such as S3, EC2, vCloud or RackCloud put your hand up. If you're using something like Google App Engine or Force.com actively right now, stick your hands up. Okay, so what about those people using Gmail, Google Docs, Office Live, Hotmail, Twitter or Facebook or something similar to that? So nearly all of you are using cloud services right now without perhaps even realising it. So my work today is done as you're all experts... As you can see, cloud computing covers a lot of different services and disciplines. Partly because of press coverage the things you expect to be Cloud Services such as infrastructure are there but so too are things you don't think of such as google maps or Gmail. The networking company F5 commissioned some research not long ago with many CTO and CIOs of large US businesses on what they considered to be THE definition of cloud computing. They trialled 6 front runners for what cloud computing is. None stood out from the rest as a sufficient or complete definition. Googling Cloud computing will give you a heap of results and nothing really convincing either. Infrastructrure Hardware guys generally believe if there's no infrastructure then it can't be a cloud. Web 2.0 Software guys like me generally take the line that it's all about Web 2.0 style applications. NIST Def The National Institute of Standards and Technology in the US have come up with this working definition. The full thing runs to a couple of pages, but here's the guts of it,. Cloud computing is a pay-per-use model for enabling available, convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and so it goes on... Obviously this is a definition by committee in order to be able to get as wide a consensus as possible NISTs definition is great for someone who is firmly entrenched in the cloud community whether as a vendor, seasoned consumer or consultant but is a bit impenetrable for everyone else coming in. So for the purposes of today let's go with something a little more straightforward. Cloud Computing Definition Cloud computing is: A Network based Service, that is available on demand such that the end consumer considers it not their problem I think it sums up the spirit at least of what NIST is trying to say. Let's dive into this a little. It's a NETWORK BASED SERVICE. Cloud computing provides a service over a network of some kind. Clouds are provided ON DEMAND. It is imperative that you can turn the service on and off as well as scale it up or down as needed. Cloud Computing is all about using a commodity so I should be able to stop using it if I need to or only use it part of the time. Finally Cloud Services are Not My Problem. Or yours. Offering the service is going to be making someone lose sleep somewhere but it shifts the provisioning, management and maintenance issues away from the consumer towards the provider. It's like flicking on a light switch. To make my lights go on I don't need to understand how the power gets across the cables or indeed connect the wires up myself each time I want to make it bright. It's not my problem – someone else looks after this for me. It's like Gas, Water or Electricity. This definition is simple but allows us to explore what cloud look like and some of the ways they can work for you right now. Flowers? Before we get into the actual types of clouds and what they can do specifically, it's worth touching on where these things live. Clouds can be Public or Private. It doesn't matter whether you are available to everyone on the internet or just one little part of it. Service clouds just made available to your business are just as important as the massive public ones that are made available to everyone. Public clouds Public clouds are those located out on the Internet and provided for a fee to parties that wish to consume them. Public Clouds are the ones that are being talked about most out in market such as those provided by Amazon, or Google. I would use a public cloud because I either don't want or don't have the expertise to run one privately. Likewise as a business I can look at a public service and think that will do the job perfectly and I just pay to use it  - why bother going and building my own? Private Clouds Private clouds are typically owned or leased by a single organisation for their purposes only. Private clouds are experiencing very large amounts of growth as many very large organisations such as banks and governments start flexing up their IT to provide more services that can be combined and consumed on demand by internal divisions. Hybrid Clouds Much like a hybrid car where you combine something like eco-friendliness with performance, it's also possible to have Hybrid Clouds As you'd expect Hybrid clouds are where businesses are using a combination of public and private components. This provides a huge amount of flexibility to the business as decisions can be taken around which services you are happy to have public and those you prefer to keep private. So now we know where these clouds are located, it's time to look at what they can do. what can cloud computing do? So now we know where these clouds are located it's time to look at what they can do. Service Remember in our definition I talked about how Cloud Computing was some kind of service delivered over the network? Well unimaginatively every time you're looking at something cloud related you'll always see it called Something as a Service. The word Service in Cloud land is almost interchangeable with commodity and Cloud Computing is really all about taking some piece of technology and providing it in the most commoditised manner possible. Cloud Computing Landscape The current Cloud Computing Landscape is made up of four major service areas- Infrastructure, Platforms, Software and Data. Infrastructure and Software as Services are currently the most well known. These have the highest uptake from consumers at the moment – Amazon's Infrastructure Services alone generates over $250M in revenue a year. However there are more services you can use as seen by platforms and Data so let's take a tour around the Cloud Computing Stack. Infrastructure as a Service This is most commonly used form of Cloud Computing currently. It's commodity hardware or infrastructure that is available for consumers of storage or processing. Server room With infrastructure you can go and put your own servers in your own room and manage them yourself or else you can go and use some else's who have built it on a drastically different scale. Usually these are paid for based on some kind of metering such as GB of storage or hours of processing time. The consumer usually doesn't need to manage the underlying infrastructure, they just put the pieces together and configure it as they like. Infrastructure Clouds have really taken off over the last 18 months or so because of the investment businesses such as Amazon, Go Grid and RackSpace are making in their User Interfaces making it quick and easy to provision storage or full servers or appliances. AWS As you can see, being able to manage your infrastructure services Go Grid Is as simple as clicking a mouse. Using Infrastructure Services. So how can I use this? With Infrastructure Clouds I can go and create everything from a single virtual server to host some site or project  all the way through to creating a virtual data centre – in minutes. Because I am a massive cloud geek I just decided one evening to create and run a Neural Network using 300 machines in Amazon's EC2 cloud – just because I could. It cost me about $50 and worked perfectly. Many Infrastructure vendors also provide storage clouds where you can upload files and pay for the storage and traffic. In practice, these services are great for trial projects or for when you need massive scalability very quickly. Melbourne cup Last year for the Melbourne Cup we deployed a Hybrid Cloud to help with web site demand during the Melbourne Cup carnival. Most of the year, the Victoria Racing Club's website is busy but manageable with pretty average infrastructure. The Melbourne Cup is Australia's biggest sporting, cultural and social event with hundreds of millions of viewers world wide. Over the Melbourne Cup Carnival, the site visits ramps up considerably with over a thousandfold increase in traffic, most of which is over only a couple of days and 90% is during about 6 hours on Melbourne Cup Day. Using Amazon's S3 service as a Content Delivery Network, we hosted all our heavy assets such as images, video and flash files. Amazon's EC2 service was used in order to deploy servers on demand as they were needed for extra capacity. Alongside this we then had a private cloud at Web Central that could be configured how we needed it based on the types of traffic we experienced. The solution proved successful with nearly a million visitors on Cup Day and at peak over 200,000 files being transferred a minute. On top of this though, it was also extremely cost efficient because VRC don't have servers sitting around doing nothing for 360 days a year as all of that extra capacity was switched on just prior to Melbourne Cup and switched off 2 days after. Using Infrastructure Services in these circumstances helps you maintain a great user experience and also makes your costs much more manageable and reduces your provisioning time. A lot businesses are using this right now to help with managing their heavy files. Look where Twitter hosts it's images... S3 Amazon Screen shot That's right – Amazon S3 Given the sheer numbers of people visiting twitter now this makes complete sense for them to be able to manage the costs of image hosting and delivery. At last count they had over 60 Billion items in there which is growing at a rate of about 10 Billion every few months so there's plenty of people using this and similar services out there. The infrastructure services are getting better all the time as competition is driving innovation. Platform as a Service Platform clouds will be immensely popular over the next couple of years. They are 1 level up from Infrastructure services and these clouds take away the ""machine"" part of the cloud giving you a platform fully configured and ready to deploy your application. Diving platform This means developers get the on-demand parts of Infrastructure clouds without having to concern themselves with managing a virtual data centre or configuring machines. Certain assumptions are made at the platform level – for example Google App Engine restricts you to certain languages you can use. However in making these assumptions, it means developer has a defined jumping off point. Google App Engine and Force.com are seeing an explosion of developers who are embracing the platform and coming up with new ways to use it. Logo Slide. At the moment platform clouds are in their infancy however they are growing rapidly. The two most well known and heavily used right now are Google App Engine and Force.com which is the platform underpinning Salesforce.com The single biggest benefit of Platform Clouds is simply Speed to Market. The development process is drastically shortened because many behind the scenes decisions have been taken for you. As such the testing time is shortened because if you're using components of the platform they are already tested so you only need to test your code not the Platform's. Additionally you also don't need to go away and build an entire system stack – databases, servers, APIs etc – it's all there waiting for you to come along and do something with it. Ongoing it means a lot less maintenance as well. Developers are looking at Platform Clouds in order to spend more time working on innovations and less time on replication. As a developer, why would I want to spend time writing yet another application that puts some customer information into a database off the back of a web form and then write some little app that allows the customer service team to make sense of it. It's this behaviour that Platform Clouds are addressing and the side effect is that not only do I get the benefits of having a complete system to work with that's developing new features all the time, I get to produce it quickly. Nucleus Research looked at 17 projects done using Force.com earlier in the year and found that speed of delivery had up to a 10 times increase. The average across all projects was about a 5 times increase in speed. Imagine what you could achieve right now if the projects your business is doing could be in market in one fifth of the time it's currently taking you? Just consider that for a second... Developers are citing similar benefits of App Engine as well as some of the other platforms out there though little official research exists just yet. As these platform clouds really mature over the next year or so the pressure will be on for businesses to start using them a lot more heavily as it will be uncompetitive to build equivalent systems from scratch. Software as a Service Salesforce.com has been the Pin Up of SaaS for a very long time – in many ways they defined what it was and probably started a lot of the Cloud Computing discussion as they showed very early on it was possible to commoditise a service and deliver it over the Internet. The reason Software clouds exist is because in the traditional Software industry there is a dirty little secret – maintenance costs more than innovation. Most Software Vendors have a small team coming up with new ideas and developing software then a massive one looking after the support and maintenance of these new ideas. Software Vendors have product lifecycles where they support certain versions of a product until they can justify not doing it any more and then hopefully force the customer to upgrade. During this lifecycle various customers could be on many different release versions and all of this adds to the cost of support. The heart of Software as a Service is that there is only ever one version of a product, delivered over the Internet and all customers use the same version until the next upgrade. Immediately much of that time used for maintenance and support is available for sales, marketing and innovation. Is it any wonder that Salesforce is now on it's way to dominating the global CRM space. SaaS Collage Most software is now moving in this direction and is a big part of what enables Web 2.0 and is transforming the way developers deliver and maintain software for their user base. Look at software like Google Docs – who'd have thought even a few years back it was even remotely possible to use a spreadheet like Excel in a web browser. Consumers of Software Clouds are generally pretty ignorant of all the underlying platform and infrastructure and are only interested in configuring the application itself. Gmail screenshot. There are many applications of Software Clouds, far too many to go through today and my advice for any business procuring any new piece of software for the organisation is to make sure you seriously consider a Software Service in your selection process. As an example let's look at Gmail. I'm guessing a lot a people here are using GMAIL for your personal email however Google has a fully supported Enterprise edition that works for your domain. You can still use outlook if you want or use the web client or mobile client if you prefer. Email for a business is as important as cashflow however for many businesses the management and in particular cost of management of email is significant. On Premises email is something you have to maintain – backups, mailbox cleanouts, archiving, system upgrades etc are all the standard tasks done by an IT manager whether employed or contracted. Even with all this time spent, email still occasionally goes down. Taylor Woodrow, a large construction business in the UK had much this scenario. They are a large business with about 1,800 employees and they were managing all their email in house using their IT team. Last year Taylor Woodrow switched over to Enterprise Gmail, moving away from their on Premises corporate email they'd had for years. Whilst there are still costs with Gmail such as licenses to pay – Gmail costs $50 per user per year - and you still need someone to administer the accounts the cost savings for the business were massive. Over the year they saved about $2 Million compared to their previous mail software. Given that large organisations have a scale of efficiency and they can get great cost savings per user, the benefits to smaller businesses are proportionately greater. For an Australian business that has between 20 and 50 employees it's estimated that many businesses will save on average about $100,000 a year in total costs due to reduced licenses, man power, backups, hardware requirements etc. That cost saving could be ploughed back into marketing, sales or other IT projects such as building your own Software Cloud. Again, this is just one Software Service example but it shows how it's possible to start realising cost savings and get a better solution. Whilst I'm sure everyone here is broadly happy with their email system, are you now or will you ever be the best email providers? Probably not. Gmail can do it better and cheaper than you can. This is the crux of most Software Services – they can be provided, maintained and constantly upgraded to a much higher standard and for less cost than you can do it for and that's why we seeing them grow so rapidly. Data as a Service Data as a Service is the newest part of the cloud computing landscape and I see this as having the most opportunity to shift how we create and consume data and is the one you can get involved in right now.. NY Times Visualisation Data is being provided everywhere and we are just now starting to see the interesting ways that people can use it. Data as a Service provides the consumer with the ability to retrieve and publish data – whether their own or that which is publicly provided - via the service. The consumer gives no consideration to the underlying software, platform or infrastructure, only whether the data is available or now. Data clouds are being used to expose data out to end consumers such as via Retail or Search APIs so those consumers can take them, remix them and provide them outwards again. Data as a Service is built for mash ups – think about all of those sites with mashups of people's tweets plotted on google maps Zappos or this great Zappo's site that shows what products people are looking at on a map showing where they are located. DaaS Applications. Data Clouds are so new that there aren't that many examples to show you yet though over the next 18 months we'll see an explosion in the data that's being made available. Organisations that are starting to do this are people like the US Government with their Open Government or Government 2.0 initiative. Lower level governments are starting to make their data available such as the San Francisco city government who are making available data sets for things like crime data, highway information, and lists of public trees of all things. Whilst these data clouds are very new we are seeing some interesting business models develop out of them. A US company called Jigsaw has recently become self funding. Jigsaw specialises in managing personal data by crowd sourcing contact information from participants and then they use that data to sell to marketers who want to cleanse their own subscription or marketing lists or who want to acquire new customers with very specific requirements. Other businesses are looking at creating industry based data clouds to manage source information such as where livestock have been reared, transported and eventually slaughtered before making their way to market thus providing a traceable and authentic source for produce certification. Layar The other area where Data Services are starting to appear is in the burgeoning field of Augmented Reality. Consumer ready AR is really being driven by and will continue to be driven by Data Clouds as huge data sets are made available to consumers via their mobile phones or PDAs. Layar for Android and soon for iPhone is one such app that allows data sets to be viewed on your mobile device. Perhaps obviously at the moment this is skewed towards things like tweets, photos from Flickr or Wikipeadia entries that are near to you but this will change as organisations start to make their data available. The examples on screen now show you where your local 7-elevens are and some points of interest in Sydney. This space is very much in its infancy right now but it's definitely an area you can get involved in massively – if you're sitting on data make services available to get at it and watch the things people start doing that you never considered. Cloud stack display again We've taken a tour around the cloud computing landscape and you can see there are a lot of areas that you can use. There are also some extremely niche areas I haven't talked about such as Desktop as a Service where your computing desktop is all cloud sourced however we'll leave them for today. One really interesting thing that I've noticed about Cloud Computing is that as you go up the Cloud Stack consumers tend to USE more of those services at the bottom to PROVIDE more of the those services at the top. There's this knock on effect of one type of cloud computing providing the means for another type to occur and it's this effect that is driving much of the growth of the newer cloud types. Now we've seen what's available let's take a look at the future to see where cloud computing is headed. Cloud Computing Future. Cloud Computing is rapidly changing shape right now. In 5 or 10 years the landscape will be as different again as it was 5 years ago. Industries and consumers will be changed by how they interact with Cloud Services. The fundamental paradigm shift that is occurring will allow IT to go back to being an enabler of business. Taken altogether, cloud computing becomes IT as a Service and businesses that embrace this philosophy will be able to do some great things. Here's some examples. Manufacturing Manufacturing will be changed by almost infinite processing power that can be provided on demand  coupled with advanced computational techniques such as Genetic Programming to explore new product designs and get competitive advantage. Health Services In health, for a long time governments in particular have been trying to make health related IT services better, providing a seamless service from one end of the health system to the other. Hybrid Cloud Services will be at the heart of our new health systems where any health professional will have proper access  to medical records so they can treat patients much better. Start Ups Start up businesses with great ideas will be able to be in market quickly, driving innovation and competing immediately with their bigger competitors, amplifying their disruptive potential within their chosen industries. Science & Research All scientific centres will have access to supercomputer grade processing and storage which will drive new discoveries rather than just those with extreme budgets. Presently a lot of cutting edge science and research is done by centres that have almost unlimited funds. Now those with much more modest budgets will be able to use On Demand and Cost Effective cloud services to help their research. New media All online sites and campaigns will be able to deal with large scale traffic and maintain high levels of availability, not just those with large budgets. Developers will be able to take advantage of this scalability and push the boundaries of the content they can deliver and the applications they can create. New Markets New markets will open by providing guaranteed access to data for consumers such as to their music and videos no matter where they are in the world – this is at the heart of services such as Hulu. Opportunities for businesses guaranteeing management and security of users personal data or industry data in so called ""Data Banks"" will start to present themselves. How much would you pay to have on demand access to all of your personal information all in one place that is searchable and secure? Geography As a side effect of all these Cloud Services becoming available, businesses will no longer suffer from geographic location. Businesses in Australasia, South America and Africa will compete directly against Europe and North America. Competition will be based on knowledge and innovation not on available capital. As you can see, all types of organisation stand to benefit from Cloud Sourcing. Warning sign Whilst I've painted a picture of almost infinite computing resources being available, great bits of software being produced and a veritable mountain of data that's about to come online it's not all rosy. With the pace of change of the technology outstripping everyone's ability to react quickly both within Government and Business, Cloud Computing is currently in a similar position to the Wild Wild Web of the early 90s. Clarity is the most important requirement for any organisation producing or consuming cloud computing services. Here's a short guide to some of the things your business needs to address before you start Cloud Sourcing. Disaster Recover and Risk Management Disaster Recovery & Risk Management are no less important than they are in your business right now! Just because clouds are someone else's problem doesn't mean the risk management and recovery issues AREN'T your responsibility. If you want 100% availability you have to work for it whether you are using Cloud Services or not. 100% availability is only possible by combining services whether at your DC or via the cloud. As an example, GMAIL had a recent outage and caused a Chicken Little style panic that ""The Cloud was falling"". Taken in context however, even with the number of people affected Gmail is still more reliable than corporate non-Cloud email which fails quite often. Disaster and Risk management are no less important just because you are using Gmail or some other cloud email service. You need to have a policy to deal with outages just as you would do normally. SLAs SLAs at the moment are the worst part of the Cloud Computing industry. Some vendors don't have any commitment, some do and there's a lot of variation between. Before committing, you need to assess the Service Level Agreement and decide if it's right for your business and right for the type service you're buying and what you're going to use it for. Some applications don't need highly available Service Levels but others do. Data policies Understanding the operational risk with regards to data is hugely important in a Cloud Computing environment as your data is probably going to be more public than you are used to. Existing data policies may need to be updated due to the additional risk of being in a Cloud Environments. Many Vendors are actually better at managing this than their clients as they are doing it day in and day out. Question Before selecting a vendor it's important to ask the following questions and understand the responses and what they mean for your business 1. How safe is your data? Who has access to the physical machines? Can the DC be cross contaminated? In large data centres it is possible for your data to be available even after you've switched off your service. Can this become available to someone else down the line? Can you access your data to move it if you want to leave? Lock in and standards around this are a massive issue that is still not completely addressed 2. How safe is your uptime? What happens if a major hub is unavailable because of some Internet issue? Who do you call when it all hits the fan at the Vendor end? How is their support provided? How do you support your consumers? Who is monitoring your services? 3. Are there any border concerns? Laws are different between Australia and places such as the US and Europe. Crossing international boundaries may bring with it additional legal requirements or may mean your services or data are subject to different laws than you are used to. Legal counsel here is imperative. Traffic light Don't let me put you off. Forewarned is fore armed and none of the issues are insurmountable with the right policies put in place by your business. If governments are using or considering Cloud Services then you are almost guaranteed that the issues with SLAs, Uptime and Data concerns are being addressed right now. Few businesses are as stringent in their requirements as multinationals and government agencies. Also, bear in mind that many of these issues should be taken into account for any of your current IT services anyway so knowledge and policies you have in place now are still relevant for cloud service provision. The important thing to remember is that you need to assess risk against the possible opportunity and  make an informed business decision before embarking on any IT project. Seismic shift. Cloud Computing is THE single most disruptive technology available to businesses right now. We are in the middle of a fundamental change in the way IT works for the business. This is being driven by the convergence of extremely cheap telecoms alongside sophisticated software and commodity hardware has led to an explosion of available computing power and storage. For the first time in our computing history we are approaching the point where actual deployable computing resources are going to outstrip our ability to use them. This creates a vast pool of cheap resource that will be used in innovative and disruptive ways. If you are not using cloud computing actively within your organisation soon you WILL be left behind whilst your competition streaks away. Ticking Clock Competitive market pressures are making this transformation occur so rapidly you have only a handful of years left to make this shift. Revolution Cloud computing is as much of a revolution as the Internet was to business in the 90s but will have occurred in less than half the time. Why? Because Cloud Computing enables not only the commoditisation of technology services but the democratisation of it as well. In the same way that cheap PCs enabled an information explosion and reformed the way we communicate, Cloud Computing changes the way technology is used by everyone. Large or SMALL. When the cloud computing revolution is complete there will be virtually NO distinction between the large and small business with regards to the scale and quality of the technology they can deploy to market. Zero The cost to deploy these technologies will be almost ZERO Time to market The time to be in market will be days and weeks not months or years. Infinite availability The flexibility and scalability of the services available will be virtually INFINITE. What can I do now? How can you start with Cloud Computing now? If you're a business get your CRM onto something like Salesforce, get your email to Office Live or Gmail. If you're a developer, get to grips with something like Google App Engine and deploy your next app there so you can build it faster. If you're about to launch a campaign, use some infrastructure services to support it from the outset using something like S3, EC2 Are you sitting on a pile of data? Make it available through a data API. Chances are if it's about products you'll start selling more of them. Here's some tips on how to sell cloud computing into your business. Cost focussed business If your business is cost focussed – and let's face it, most are at the moment – Cloud Services are Operating Expenditure rather than CAPEX. There's no depreciation, financing or generally up front costs. You pay for what you use and nothing more. Cloud services are generally cheaper than maintaining Infrastructure, Platforms and Software of your own. Providing Data as a Service might lead you to discover additional revenue streams or result in more sales of your core product. Process focussed business If you are a process oriented business then you need to look at starting with small, low risk trials and projects. Look at primarily Software services for a starting point. Make sure you do your homework around SLAs & legals. Most Cloud vendors realise there are issues and are attempting to fix them. You can also consider setting up your own private cloud. VMWare, Melbourne IT, GoGrid and Rackspace all do this and I'm sure would be very happy to help out. Innovation focused businesses If you are an innovation business the key thing to stress is that you can be in market faster by using Software Services or developing on Platform Clouds. Your focus needs to be on how you can innovate using these tools and how they stop you from having to replicate and go over ground that has been covered many times before. For innovation businesses you can trial things out with much less commitment. If the results are poor, switch off the service and the costs stop. If you have a hit you can scale up quickly to accommodate your new market. Innovation businesses will typically combine many kinds of services together in novel ways. Experience across all parts of the Cloud landscape will provide significant competitive advantage in market. Last Slide Hopefully this presentation has helped provide some insight into what Cloud Computing is, what it can do and what you need to be aware of before jumping in. Cloud computing IS a fundamental shift in the way business works. Cloud computing is not about a single technology or a single service. All of the elements come together differently for each and every business. Cloud Computing is primarily a shift in the way of thinking about IT and how IT can enable the business to radically transform the way it operates. Those businesses who embrace Cloud Computing will have a significant business advantage, limited only by their ideas and their willingness to execute on them. How are you going to Cloud Source your business. This article was cross posted to the [Citrus Agency Blog](http://citrusagency.blogspot.com/2009/10/presentation-web-directions-south-2009.html)","['business', 'cloud computing', 'conference', 'presentation']","['business', 'presentation', 'conference', 'cloud computing', 'web']","[0.605981113735297, 0.5803733992398715, 0.5757567821143675, 0.5727272511121114, 0.3362970874180066]",['web'],[],[],[],True,4
2009-10-11-the-only-reason-why-linux-isnt-ready-for-prime-time-desktop.md,One feature away from desktop Linux,"['desktop', 'linux', 'os', 'presentation']","One feature away from desktop Linux
Okay, so this title's probably a bit misleading as there are probably a few reasons but as far as I'm concerned there's only one thing stopping my final transition to desktop Linux for complete every day usage. Presenting In my job I do a lot of presenting. I give major milestone presentations on projects, I present to the business on things that are going on, I present in pitches where we are attempting to win new business and recently I've started presenting at conferences. I would not use my Linux desktop (and I have combinations of [Ubuntu](http://www.ubuntulinux.com/) and KUbuntu 9.04, CentOS) to present with at all - even if someone paid me. Before I say why I'll also lay out my Linux credentials. I use RHEL, Ubuntu and Centos EVERY day. All of my home computers are Linux based, I have a Linux PDA, I prefer my Ubuntu desktop for work and I administer numerous Linux (CentOS and RHEL) servers - via command line - all of the time. I've used it for over a decade and am more than happy with it and more than happy to hack on it to get stuff working. However, there comes a point where I am not going to entrust a complete presentation that our business or my reputation relies upon to Linux's extremely flaky graphics system. Yes, I know laptop Linux is problematic (but if the rest of the desktop is stable why not my second video out?) Yes, I know that graphics card support (particularly from ATI) is very closed so there's lots of reverse engineering going on (but again if I can have one video out working why not two?). I'm not sure why this is the case - I think it's a combination of X.org config and poor tools for configuring multiple screens with different resolutions but it definitely needs a lot of work to go ready for prime time. I was at a [conference this week](http://south.webdirections.org/) and I had built [my entire presentation](http://technologytreason.blogspot.com/2009/10/web-directions-south-2009-cloud.html) in my Gnome desktop using FLOSS tools like [Open Office Impress](http://www.openoffice.org/), had a great looking presentation and was legitimately keen on presenting using either my Ubuntu or Centos desktop. After hours of mucking around however I didn't feel supremely confident in just walking up to the podium, plugging in my laptop and ""It Just Works""TM. It's just too hit and miss. I don't generally experience this with Linux in general and Ubuntu specifically although I am aware of other people saying it. For me 99% of the time it does actually just work. So I defaulted back to my dual-boot Windows partition and presented from that instead. This was the partition that I had considered nuking because I hadn't used it in about 6 months. In this instance though I didn't have any other choice - and sure enough it did just plug in and go. I still presented from Open Office Impress though (which is a fantastic bit of software I might add!) and I think I was the only one at WDS09 that presented with it (and I'm sure no one could tell I wasn't using PowerPoint or Keynote). Desktop experience is exactly that - an experience and our experience, particularly when we are doing something social with a computer can affect our mental state quite substantially. If I'd have taken the decision to present using Ubuntu I would have felt worried about whether my laptop would work and I would have been nervous and probably would have delivered a terrible presentation. In contrast because I knew I wasn't going to have any support issues I felt confident, in control and delivered what I hope was a good presentation to the audience. Ubuntu are trying to address many of these issues with the [Paper Cuts project](https://launchpad.net/hundredpapercuts) but that's really aimed at business. Apple have addressed similar issues (hardware compatibility) by having a presenter's kit (which you buy) which provides all kinds of adapters to go from Mac to just about every video input type. Microsoft addressed this years ago from Windows 2000 with a great set of dual head tools that made it simple and a standardised way for vendors to incorporate them and it is extremely rare for it to fail. Business use is one of the areas that Linux (and especially Ubuntu) has got a real opportunity to shif users across as there are so many other business benefits but users want a single consistent desktop so they aren't going to build on one desktop and present on another - it's too inconsistent. For me this issue on presenting and graphics support isn't so much a paper cut as it is a gaping flesh wound and it really needs to be addressed.","['desktop', 'linux', 'os', 'presentation']","['os', 'linux', 'desktop', 'presentation', 'web']","[0.6140199460379999, 0.6055677787930666, 0.5930234436627005, 0.5905234269949958, 0.32889941732757966]",['web'],[],[],[],True,4
2009-11-09-admob-purchase-by-google-paves-way-for-interesting-developer-funding.md,AdMob purchase opens door to new funding models,"['business', 'google', 'media', 'mobile']","AdMob purchase opens door to new funding models
It's just been announced that [Google](http://www.google.com.au/) is set to buy [AdMob](http://www.admob.com/) for $750M in an all-stock deal. This is the third biggest purchase Google has ever made (the only two bigger are YouTube and DoubleClick). AdMob started in 2006 so they have capitalised very well for a 3 year old business. Indeed they've been cash positive for a while now so this is a great acquisition by Google. The full gory details of the deal [can be found here ](http://news.cnet.com/8301-30684_3-10393623-265.html?tag=mncol;txt) and a [press site by google here](http://www.google.com/press/admob/index.html) We know this is all aligned to Google's interest and in particular their big appetite presently for anything Mobile. However this also opens up some enormous opportunities for developers. This acquisition brings with it some great opportunities for in-application display advertising that is delivered contextually but also based on Google AdWords auctioning technology. Along side this I can then use the same advertising account to drive ads on my mobile website that compliments my application and then use standard ads on my main website that provides additional information / community support etc. All of a sudden a possible revenue opportunity opens up that was kind of there previously but wasn't very smart. Over the last 18 months in particular we've been watching the rise of free-ad-supported applications as well as paid-no-ad versions of the same application. I would expect to see a lot more of the ad-supported apps once this deal goes through. The reason for this is twofold: As a developer I can manage all of my advertising spaces with one vendor. I don't really want to have to deal with all these businesses I just want to get some beer money for my app that I'm spending my non-work hours producing. With contextual ad serving, I can make certain elements of data within the application available and use that to generate calls to the Ad Server - much the same way AdWords works with a web page or in Gmail. This means the ads that are served will be more relevant to the content which should lead to higher Click Through which then leads to potentially more revenue for me (see note above about beer money) This makes a lot of sense for an advertiser as well. Certain applications have huge amounts of uptake - [twitterific](http://iconfactory.com/software/twitterrific) on iPhone or [Twidroid](http://twidroid.com/) on Android for example. Imagine having contextual ads served based on the content of your twitter stream. Twitter might resist it but it could make some serious cash for the app developers. Overall I think this will really blow the top of mobile advertising. Advertisers who have been a little shy in the mobile space will be comforted by the fact it's Google doing it. App and mobile site developers stand to gain some good funding from it and it be relevant for their audiences and as the world goes increasingly smartphone mobile mad over the next 18 months this will be worth serious $Billions in the next 5 years or so. Cross posted to [Citrus Agency Blog](http://citrusagency.blogspot.com/2009/11/admob-purchase-by-google-paves-way-for.html)","['business', 'google', 'media', 'mobile']","['mobile', 'business', 'media', 'google', 'web']","[0.6454272996502394, 0.622987860887465, 0.620310026103419, 0.6137124555561858, 0.3376194949424932]",['web'],[],[],[],True,4
2009-11-13-spdy-could-gain-acceptence-very-quickly-with-some-product-innovation.md,SPDY could gain acceptence - with some innovation,"['google', 'internet', 'web']","SPDY could gain acceptence - with some innovation
Google have announced some early findings about their research into a faster protocol to reduce latency times due to good old fashioned HTTP. HTTP was designed as a really simple protocol to delivery (primarily) text content over the Internet and thus was born the Web. One of the problems with HTTP is that it only really allows a single request to be serviced at any one time. The reason this doesn't APPEAR to be the case is because modern browsers create multiple connection threads that connect independently to the server and it gives the appearance of things downloading in parallel. It's a neat hack and works because we have good network speeds and mast processors to manage all this multi-tasking. Go back to a Pentium II with Netscape 2 and you'll watch the glacial procession of elements loading in from the top and goes down the page. The [Google project page ](http://dev.chromium.org/spdy/spdy-whitepaper)talks a lot about why HTTP pipelining doesn't work and some of the technical architecture behind SPDY which I won't cover here other than to say that it's great we are seeing this type of innovation at the protocol level. What's most interesting for me however is how we get it in production. There is a lot of nay-saying going on around this suggesting that because of the size of the Web you'll never get people to shift to a new protocol HTTP:// won, let's all leave it at that because there are too many web servers and web browsers to convert. This is what I want to address in this post. Yes - there are fair too many legacy browsers to deal with to make this transition happen. Look how many IE 6 browsers are still in use, but we'd also have to shift all the Mozilla users, Chrome users (easy because of forced update) and Safari users as well. Not to mention all those pesky mobile devices that are springing up. Dealing with the web servers is a much more straightforward issue. There really aren't that many in the scheme of things. Indeed much of our existing infrastructure runs multiple servers, Apache alongside a lightweight server like nginx and this is increasingly common. As such there's nothing stopping me dropping in a SPDY server alongside my existing infrastructure for those users that can directly access it (Chrome 4, Firefox 5, Safari 6 and IE 10 for example). But let's not stop there. A network admin could create a software appliance at the Firewall or Internet Gateway level for the corporate network that took HTTP requests, turns them into SPDY requests and then proxies these back. Now I have doubly fast Internet connectivity without upgrading my connection. For the price of a box that is well worth it. For home users we could do the same thing. This protocol is software - it runs on TOP of TCP so because of that a Firmware upgrade of your average Netgear or Linksys home router could get you the same benefits as those above. ISPs could force this remotely on certain systems (Cable for example) or provide info on how to do it such as through a web, phone or personal service. So for all the nay-sayers out there - this is a MASSIVE opportunity to speed up the web and people need to think outside the browser sometimes. QoS was delivered at the router level based on intelligent packet analysis - that speeds up network traffic massively but it's a software change not a hardware one. I don't think it will be long until we see Netgear and Linsys start promoting this like they did with the WiFi standards and force adoption because it makes a great marketing case to do so. I'll be trying this out at the rawest state to see if we can make it work and if I can, watch how fast our servers and network gateway get upgraded before I embark on upgrading client machines.","['google', 'internet', 'web']","['web', 'internet', 'google', 'development', 'mobile']","[0.643091623245334, 0.6263625146245515, 0.5892216821328576, 0.32381641855020415, 0.3128834332775088]","['development', 'mobile']",[],[],[],True,3
2009-12-22-why-im-interested-in-aws-spot-prices-for-ec2.md,Why I'm interested in AWS Spot Prices for EC2,"['cloud computing', 'development', 'devops']","Why I'm interested in AWS Spot Prices for EC2
There's been a lot of chatter going on around the intertubes over the last couple of weeks since [Amazon Web Services](http://aws.amazon.com) released their [Spot Instances](http://aws.amazon.com/ec2/spot-instances/) pricing model for EC2. In a nutshell - AWS have created a compute market. Instead of charging the same price to every person for the same product they have basically created a market where people can buy compute time at less than a price they are willing to spend based on the current demand. There's been some conversation about the fact everyone should just put the current demand price in as their maximum and this would game the system ([the comments here for example](http://gevaperry.typepad.com/main/2009/12/amazon-ec2-spot-instances.html)) however this misses the point slightly. The [Clouderati](http://twitter.com/#/list/ajfisher/clouderati) often talk about Utility Computing or Commodification as one aspect of Cloud Computing and what AWS have done is the logical conclusion of that - they have created a true market for the provision of computing time based on supply and demand. Now what's interesting with the ideas some commentators have come up with regarding gaming is it assumes everyone's working the interest of everyone else. That isn't the case. Yes I know I could get some resource cheaply if I keep my bid low and am willing to wait for a period of time however I have clients and they have deadlines. That big compute job crunching all the marketing data needs sorting out this afternoon - so I'm going to put a high bid in for 50 nodes NOW! The market will accommodate that and those with low bids will be knocked off. Thus the market constantly corrects to the requirements of demand. But it's the flipside of this which makes me really interested in EC2 Spot Instances. I can have a battery of servers doing work at little to no cost if I build my system correctly. The critical element to this is I need to address availability correctly - that is I need to ensure that my entire system doesn't go down because I've been priced out of the market. This is a really rough idea at this point but I'd love feedback around it - it's obviously based around some kind of online application that requires multiple nodes. I have an instance which is the master. All parts of the stack could be retreated back to this server if it's needed. I have Cloudwatch or some other monitoring system assessing the performance of my nodes so I can see when I have spare capacity or when I'm under utilised. The master server has a series of heuristics looking at the current work loads and the current costs that each server is incurring versus the work it is carrying out. Thus low utilisation and low cost is okay but low utilisation and high cost would cause alarms to go off. The heuristic set up makes reference to the demand pricing level and strives to always keep each instance below that price. As the spot prices go up and over the demand price I immediately terminate expensive spot instances and start replacing them with lower price demand ones. As the price comes back under then I can replace demand prices with spot prices The master server then creates instances as required to fulfill the work units that are required and link them into the system. Each node is able to be switched off mid unit so the entire network is self-healing So the only thing that would be required to get this up and running now is having a reliable system for creating nodes and getting them working into the network as quickly as possible and producing the heuristic system to monitor and create and destroy instances based on some rules that would create some intelligence around pricing. Not least the system would need to determine whether a mix of different types of instances would be appropriate if there are large distinctions between their current spot price for given work units. For example: If we were serving a bunch of web pages using some heavy duty memcached system then RAM is the most important commodity. Say I have an instance of 1.7GB RAM at 3C/hr and another instance of 7.5GB RAM at 15C/hour then my intelligence system needs to understand the component (Memcached) just needs buckets of RAM and that getting 5 instances at 3C/hour is better value than 1 at 15. Importantly it can then ramp up towards that number based on what is actually required rather than doing the whole lot and then under-utilising. So I think we're quite a way away from this type of system but my opinion is that this isn't out of the realms of possibility and importantly the market Amazon has created has allowed (I could almost say ""is going to force"") these types of architectural considerations to start being made. Interestingly all of a sudden decisions I am going to make around infrastructure is going to be much more value based. It's not about ROI - it's about value and am I getting the best value from my infrastructure. IT teams that get this are going to make an absolute killing with the type of services they can offer and the prices they'll be able to do it for. Am I off my rocker? I'd love to explore this idea further.","['cloud computing', 'development', 'devops']","['development', 'cloud computing', 'devops', 'web', 'internet']","[0.6311058318468327, 0.5905691183961944, 0.5807181242028582, 0.3327712157917601, 0.3080131063818196]","['internet', 'web']",[],[],[],True,3
2010-10-07-i-like-where-i-think-sony-is-going.md,I like where I think Sony is going,"['android', 'consumer electronics', 'mobile', 'predictions']","I like where I think Sony is going
This is one of my speculation fueled posts so I'm admitting that a lot of it is based on rumour, hearsay, wild prognostication and adding up 2+2+2+2 to equal 63. With that in mind on with the rest... Right now I'm inclined to think that [Sony](http://www.sony.com) is on the cusp of a resurgence to being the most dominant player within the consumer electronics space in the world. This is a contentious point because Sony certainly represent well within every part of the CE market from computers, games consoles, TVs and phones through to DVD players and handhelds - it's not like they are at risk of disappearing any time soon. However if you look at Sony's position in these markets they are slowly slipping places, losing ground to Samsung (TVs & DVDs), Apple (Phones, Media Players), Microsoft (Consoles), Nintendo (Handhelds, Consoles) and others. However they have a huge opportunity and I think they've realised it but the rest of the world hasn't yet. The advantage Sony has over every other company I mentioned above is they are already present in every single part of the Consumer Electronics market. They also operate in channels the others don't touch - distribution and content creation. Whilst Apple might have iTunes, they still have to license the movie from Sony Pictures to distribute it. Whilst Samsung has the beautiful 50 inch LED display with a BlueRay player, they still have to get the Discs from Sony to play on it. Sony's amazing opportunity lies with their phones, and in particular their [Android](http://www.android.com/) phone platform - the [Xperia](http://www.sonyericsson.com/cws/products/mobilephones/overview/xperiax10) series. The X10 has been a big hit for Sony this year. A contact of mine from VHA says that the [X10 Mini](http://www.sonyericsson.com/cws/products/mobilephones/overview/xperiax10mini) has been selling really well even though it's a small phone with Android 1.5 - it has introduced a lot of new customers to Android and Sony phones. Couple to this Sony's [announced release of Crackle](http://latimesblogs.latimes.com/entertainmentnewsbuzz/2010/10/sonys-crackle-movie-and-tv-streaming-service-debuts-on-android.html) - their new content streaming service that will allow you to watch TVs and Movies on your phone and you can start to see their play. HTC showed in the middle of the year though that the future of the phone was one coupled to big displays. The [HTC Evo](http://www.htc.com/us/products/evo-sprint) can shoot high-def video and has a mini HDMI connector so you can throw your phone display onto the big screen - perfect for looking at pics, video or gaming. Coupled with a fast processor and a stunning display of it's own and you can see why it was one of the fastest selling phones in 2010. Many of the new top end Android phones are starting to follow suit so expect to see HDMI feature on Sony's next high-end Xperia and you'll be able to watch HD movies streamed via Crackle to your home system. With bandwidth detection it would be simple to be passing down 5 or 6.1 sound for your home theatre as well and HDMI will take care of the connections. Sony have dropped a few hints in public over the last few months about slates running Android and clearly they will be watching the [Galaxy S Tab](http://www.samsung.com/au/smartphone/galaxy-tab/) to see if they can outdo them. A mobile device that can play to BlueRay quality would be very cool and would outdo the iPad. Finally there's gaming - there have been some [persistent](http://www.engadget.com/2010/08/11/exclusive-sony-ericsson-to-introduce-android-3-0-gaming-platfor/) [rumours](http://www.google.com.au/images?hl=en&source=imghp&biw=1333&bih=626&q=psp+xperia&gbv=2&aq=f&aqi=&aql=&oq=&gs_rfai=) flying around that the next Xperia phone will be Sony's replacement for the PSP. I subscribe to this not because I love my PSP but because it's unlike Sony not to have shown us anything new for so long. The PSP Go last year wasn't a roaring success but it gave Sony experience working out how to distribute games digitally and assess the state of the market. With no hardware upgrade and no new games there wasn't really a compelling reason to upgrade. PSP is now at the end of it's life cycle. The other reason I subscribe to this is that by the time we get to the next iteration of the Xperia it should be possible to drop an emulation chip onto it to play PSP games natively without significantly affecting battery life or hardware size (Sony have a long history of this by putting in hardware emulation for PSOne and PS2 platforms to maintain backwards compatibility on old games). I've seen a few offers over the last couple of months where ""free"" PSPs have been offered to customers buying an X10. I don't think this is because sales are poor, it's an incentive to finish off the stock of PSPs and get people used to PSP games ahead of a move to getting them via your phone. With the current crop of graphics chips being designed for mobile devices and the speed improvements on CPUs (most new Android phones are running at least a 1GHz [Snapdragon core](http://en.wikipedia.org/wiki/Snapdragon_(processor))), gaming is becoming a first class citizen on mobile devices and many now far outstrip a humble PSP or Nintendo DS and are even challenging a Wii or XBox in terms of power. Combined with a couple of wireless controllers (Bluetooth naturally the same as a Wiimote) and you've got a gaming platform you can throw on the TV and take with you on the move. Pretty compelling for casual and hardcore gamers alike. I stated at the outset of this post that there's a lot of wild speculation in here but it's mixed with a lot of hope too - this is EXACTLY the way I want my home to run - my phone should be the most useful and flexible device available, being able to syndicate services to other devices to enhance it (display, processing, storage, control etc), it needs to be the hub that everything else hangs off. I don't think there are many other businesses that can do this. Samsung could have a go but they haven't got the gaming heritage nor the content to pull off a total vertical play (though partnerships could be had), Apple clearly don't have the games or the TV technology to do this (Apple TV is a dead end in my view anyway and is just a pretty Tivo - Sony could render this irrelevant). Nintendo simply don't have the interest. If Sony aren't doing this then I'll be very disappointed. If they are then I can't wait until CES 2011 as we have to be right on the cusp of some announcements around this. My prediction - the Christmas of 2012 will have a lot of Sony boxes as consumers re-envision the way their home entertainment works and finally unify all of these devices using the humble mobile phone as it's core.","['android', 'consumer electronics', 'mobile', 'predictions']","['mobile', 'android', 'predictions', 'consumer electronics', 'internet']","[0.6622445190058373, 0.6135164675735042, 0.607917765511124, 0.5885393093758067, 0.329914378142358]",['internet'],[],[],[],True,4
2010-10-18-how-to-avoid-the-audiencepresenter-disconnect.md,How to avoid the Audience : Presenter Disconnect,"['essay', 'presentation']","How to avoid the Audience : Presenter Disconnect
I've just finished two fantastic days at [Web Directions South](http://south10.webdirections.org/), a conference that has great organisers, great participants and largely informative and inspirational presentations from experts in their respective Web disciplines from around the world. There's a full wrap up coming shortly with my views on where we're going and the biggest topics of the conference but that's not what this post is about. Most conferences these days have a twitter backchannel usually revolving around the hashtag of the presentation or the conference. For Web Directions this was [#wdx](http://twitter.com/#!/search/%23wdx). This means the conferees can focus their messages and you end up with conversation and often with a [semi-live transcript](http://wthashtag.com/transcript.php?page_id=19429&start_date=2010-10-13&end_date=2010-10-16&export_type=HTML) of the presentations with insights that people not attending the conference can view at the same time. Over the course of WDX the backchannel was largely positive with nearly 3000 tweets in two days from over 500 contributors. This highly engaged audience is much more ""tweet-happy"" than any other conference I attend over the course of the year. At two points during the conference though, the mood turned bad - the closing keynotes of both days. The backchannel created a feedback loop of negativity that created a unruly mob. This isn't unique to WDX, I've seen it have a few times now - a real time back channel can significantly amplify the point where a speaker loses their audience and this is what I call the Audience:Presenter Disconnect - the moment where the two occupy the same space and time but two entirely different activities are occurring. At this point I'm going to point out that I am a speaker, I've presented at [Web Directions previously](http://www.webdirections.org/resources/andrew-fisher-cloud-computing/) and some other conferences large and small. The thought that an audience could turn on me like this makes me feel sick thinking about it but hopefully this gives me a perspective that can explain the behaviour and provide thoughts on how to combat it as a speaker. Why the back channel turns feral: The backchannels can turn feral for a variety of reasons, but here are some of the main ones I've seen. Idleness I've seen this happen in numerous presentations and is simple ""Idle hands doing the devil's work"". This sounds like a ridiculous explanation but I think it counts for a lot. Conferees come expecting to mostly be informed - entertainment is a bonus but mostly it's about information delivery. I sat through one of the most awful presentations of my life at the CIO summit earlier this year and no one turned on the speaker because as bad as his presentation was, his information was brand new and clearly understood by the audience. If you are providing a solid stream of information, your audience won't have time to do anything other than process the data and concentrating on shaping your messages into 140 character bursts of insight and throwing them to the wind. A great example of this was [Simon Pascal Klein's](http://twitter.com/#!/klepas) presentation on [Web Typography](http://south10.webdirections.org/program/design#setting-standards-friendly-web-type). His presentation wasn't the most entertaining but it was one of the most information dense of the conference but was paced well. The volume of messages flying out was consistently high during the whole session. The overall sentiment was of extreme positivity and he will get a lot of deck downloads afterwards. [Dan Rubin](http://twitter.com/danrubin), [Dmitry Baranovskiy](http://twitter.com/dmitrybaranovsk) and[ Matt Balara](http://twitter.com/MattBalara) were other positive highlights because of the amounts of information packed into their presentations. If your presentation lags in terms of its information density people start thinking ""why am I here?"" and start saying as much. Misjudging the audience At any technical conference, be it Web, engineering, medicine or otherwise a speaker should never underestimate the knowledge of their audience. A conference is a place for people of similar interest and skill level to swap information and learn from others so the whole group improves over time. The biggest danger a speaker has is pitching low - better to go high and force some of the audience to ""step up"" their knowledge afterwards rather than go low and appear patronising. Dmitry Baranovskiy and [Steve Souders](http://twitter.com/souders) both did excellent jobs of this - loads of people walked out of their sessions saying ""wow - that will take me a while to digest"" but no one walked out saying ""I knew all of that, what a waste of my time"". As a speaker at a technical conference you are either there to inspire generally such as Scott Thomas did with his Designing Obama opening keynote or more likely you are there delivering knowledge as a ""First amongst equals"" like Dmitry, Steve Souders and [Daniel Davis](http://twitter.com/ourmaninjapan) did. The best presenters I've seen do this (and all these three did it) and take the attitude of, ""I found something you might think is interesting, check this out"". I spoke to Steve Souders after his talk about some of the content and his results. Even one-on-one I found him to be extremely humble and willing to listen to some ideas that might help him from some random guy even though he's written the books on site performance. Everyone is a specialist at something but in the development and design of the web we're all technical peers and behaving like one gains a lot of positive sentiment. The feedback loop The twitter backchannel moves quickly, if you are sitting in the audience with a laptop or iPad (that is, any device where you can consume and contribute very quickly - phones are a lot slower in this regard) the conversation is reacting in as close to realtime as possible with only seconds passing from a speaker making a point or displaying a slide and feedback appearing upon it. Again, where the speaker is informative or entertaining you see a feedback loop of positivity - someone tweets quickly and a slew of people retweet the original or add further comment. [Ben Schwarz](http://twitter.com/benschwarz)' presentation on Thursday had a lot of this - he was making some fairly controversial statements about the W3C and as it was delivered well he received an immense amount of ""yes I agree"" style responses. The audience was onside and he turned that into action by illustrating how he stopped bitching and started doing - launching the [HTML 5 spec for Web Authors](http://dev.w3.org/html5/spec-author-view/) during his presentation, possibly the most highly shared piece of content of the conference. We had to follow his actions with our own. Contrast this with the closing keynotes. Once people started making negative comments, more people retweeted or chimed in on the ""I agree"" bandwagon. One of the presenters got a lot more pointed criticism at his presentation because the audience was mis-sold. The session was ""Where are we going?"" but spent 30 minutes on ""What has influenced where we are going?"". The audience got grumpy. To [Josh's](https://twitter.com/JW) credit his last 10 minutes were inspirational and gave us a future vision of location services - the audience rallied around him and he managed to largely turn them back onto his side by the end. The backlash backlash The feedback loop is a fickle thing and it can turn positive or negative very quickly. An interesting side effect is the ""backlash backlash"". This is the equivalent of a [Cnut](http://en.wikipedia.org/wiki/Cnut_the_Great) moment where several people start trying to stop the negative loop by saying ""hey it's really hard being a speaker give the person a chance"" or ""I don't see you standing up there - put up or shut up"". Unfortunately this typically fans the flames and it was interesting to note that neither Maxine or John from Web Directions waded in during the presentations or afterwards which is a great approach for the following reasons. As a speaker if I screw up I'd like to know about it afterwards. A tweet transcript is a great way to get feedback about a presentation. I can review it over time compared to my deck and see what was a problem. Likewise, I have the right of reply. If someone's said something you can bet I'm going to answer them later. Because of it's immediacy, even with the dangers of the feedback loop, twitter is a very good release valve. On stage you can feel the audience's attitude, whether they are engaged and watching you or whether they are talking amongst themselves. [Scott Berkun](http://twitter.com/berkun) in his book [""Confessions of a Public Speaker""](http://www.amazon.com/gp/product/0596801998) talks about this a lot. This creates its own feedback loop as positive audiences provide energy to the speaker but negative ones sap it. In my view Twitter provides a valve that stops negative feedback washing onto the presenter and causing them to do worse. This means that at a high point the speaker can still turn the audience onto their side and get the positive effect rather than going flat. Josh Williams' presentation showed he had ""some left in the tank"" towards the end and finished very well because of it. What can you do as a speaker? As if standing in front of several hundred people wasn't harrowing enough (I even get physically sick from nerves), as a speaker you now have the backchannel to worry about and whether or not you'll be eviscerated on twitter. Below I've got some thoughts about what can you do to stop the Audience:Presenter Disconnect and create positive feedback in the back channel. Focus on speaking not the back channel One of things I love about Web Directions is that it doesn't display the twitter stream during sessions. This stops the feedback loop going out of control and influencing non-tweeters in the audience and allows the speaker to focus on delivering their presentation not splitting their attention between the two. If you are presenting, find out whether the feedback loop will be shown and if you aren't comfortable that you can zone it out of your awareness insist it's turned off. Say to the audience you have a really low attention span and you'll watch it instead of delivering your presentation - they won't care as they can still see it via their clients anyway. Engage the back channel before the session [Craig Mod](http://twitter.com/craigmod) gave an excellent presentation on how Digital affects books & publishing. One of his most interesting ideas was this notion of the ""Immutable Artifact"" - in his case a book. I would suggest a presentation is an Immutable Artifact; it's almost impossible to change once you've started delivering as it's scripted and tied to a deck. However, Craig talks about how the lines of the immutable artifact can be blurred by getting contributions before it's creation. This can be done through various channels beforehand asking what people want to know about before you construct your presentation. Afterwards it's about taking feedback and commentary and working it back in whether by adding appended notes when you upload it to Slideshare or by reworking parts for your next presentation. Conversing with your audience before hand connects them to you more closely and gives you humanity through prior engagement - it's harder to lay into someone you ""know"". Deliver information first, entertainment second I made the point earlier that I've seen some horrific presentations but they were so information packed and thus valuable. Any presentation that is remotely information led, imparting knowledge from your brain to your audience's with some nuggets of insight should put the information delivery first and worry about the entertainment second. Some presenters are witty and full of energy, bouncing around on stage and can talk without notes. At WDX, [James Bridle](http://twitter.com/stml) did this, so can John Allsopp. If you can't present like this, don't even try. I am so nervous during a talk that even if I had a perfectly scripted joke it would come off flat and fail dismally. Steve Souders' presentation was information dense, it wasn't comical or witty but it was an outstanding presentation and his is well worth watching and learning from in terms of style. Contrast this with one which has very little information density and is trying to be entertaining by showing videos and funny pictures or cracking jokes. It comes off poorly for being vacuous and the audience feel ripped off for having sat through it. At a conference the audience is paying for the speaker to inform them. Entertainment is a nice surprise when it happens. If I want to be entertained by people on stage I'll go to a concert, the theatre or see some stand up comedy. Conferences are a different context and one that all technical presenters should consider when creating content. Delivery of information will stop the ""Just get on with it"" or ""Tell me something interesting"" type chatter in the back channel. If you create a steady stream of points people will be so absorbed taking notes and passing on your insights they won't care if your slide is out of alignment, you fumble some words or haven't told a joke. Consider points that can be tweeted I hate this idea as it's a sign of our waning attention spans but it's a fact of life so you either roll with it or stick your head in the sand. Our communications are turning into microbursts of activity. When you take notes at a conference you note the points not the words, when an audience member tweets, they compress and express the insight, not all the details. If you are taking a long time to get through a point or have made the same point ten times the audience is going to start getting frustrated as they have stopped annotating your talk. This is complex because sometimes you have to build a story to illustrate a point. The challenge is to get there quickly and avoid the ""I wonder where this is going?"" style comments. Something I've started doing recently as a result of observing my own behaviour at conferences is reviewing every slide I'm presenting and expressing it as a tweet or short annotation. If I can't, or it takes me a paragraph to express then it's a problem and I can try and fix it. If your presentation has gone more than about 3-4 slides (about 2-3 minutes) without something tweetable or notable then you're on the limit of the audience's attention span wavering. Unfortunately, Tim Harrison suffered from this heavily - in 50 minutes I tweeted only a few points he'd offered and didn't write any notes. Contrast this to Simon Pascal Klein's presentation where I sent 20 tweets AND wrote 3 pages of notes! Craig Mod similarly scored 15 or so tweets and a few pages of notes as well. We live in an age of soundbites and 140 character points - make it easy for your audience to rebroadcast your message and they will. Don't go silent This is a strange one that I've just realised due to seeing a series of videos used in a talk. Video is a powerful medium if it's used well and I've come to the conclusion that the only time you can use it is if you are providing a commentary or it is extremely short (I'm guessing less than 20 seconds). Steve Souders and James Bridle did excellent presentations incorporating video - not least because they were showing things that were relevant to the presentation but they were annotating the moving image with their own insight. Scott Berkun writes about the moment when hundreds of people channel their energy directly at the speaker - which can then be harnessed into your presentation. Directing that at a 3 minute video when you're not speaking squanders that energy and turns your presentation into a cinema experience. Challenge the common view and provide more than Google One of the most common tweets to start a negative backchannel looks like this - ""You need to consider X when you do Y - yep we've all got it lets move on"". This often happens when a speaker dumbs down for their audience who consequently get bored of a point that was made in 30 seconds but is taking another 5 minutes to deliver. Assume that over half the audience know close to as much as you do in your topic area. You're the specialist but only by a small amount. Also assume that the other half are more than capable of understanding what you're talking about and will be inspired enough to go and find out more because of the information you've delivered. If you follow these two assumptions then you won't belabour a point or regurgiate information that can be found with a google search. Ben Schwarz' presentation on HTML 5 showed this beautifully as the information he delivered in the second half of his talk was almost impossible to find. In 20 minutes he aggregated months of work into simple points everyone could take away and research further. Dan Rubin and Steve Souders did the same thing - is it any wonder their resource links on their last slides were some of the most shared pieces of content of WDX. A great way to avoid this is to test your presentation on other people. Steve Souders' presentation showed this - it was honed so there was nothing superfluous. User testing is always a worthwhile thing to do. Don't disrespect the audience This goes without saying but it's a minefield. Disrespect can come out in a lot of different ways, lack of preparation, appearing bored or not giving your audience enough credit. Again, testing your subject matter and the way you're going to deliver is well worth it here. I've pulled slides numerous times because of feedback that it was pitched too low. Conclusion There are plenty of much better speakers and presenters than I am or ever will be - I work with at least two and I am in constant awe of people that can deliver amazing presentations effortlessly to any sized group. Conversely, I don't think I'm the worst speaker or presenter ever - mostly because I am so nervous that I don't lack for preparation - I literally have backups of backups of backups to account for things going wrong. I don't know whether the things I've written about here are right, or if they are right for you. They work for me, it's how I approach a presentation and if they help you out too that's great. I think I have a unique perspective because I present and also a vocal and active member of the backchannel at any conference - it's what I do. I think the backchannel is a great method for freeing information and catapaulting it outside of the conference hall, promoting the work of the presenter to others that couldn't be there who can in turn pick it up and amplify it. An audience doesn't come to watch a presenter fail. They come to be informed, to hopefully be entertained but mostly to go away feeling as though the presentation was worth the money it cost and the time they invested to view it. Twitter can be a powerful amplifier of your message if you can get the backchannel working for you - unfortunately it can also go negative but at least now you know the reasons why and how to prevent the Audience:Presenter Disconnect from happening before you've even stepped onto stage.","['essay', 'presentation']","['essay', 'presentation', 'web', 'media', 'internet']","[0.5953049591261219, 0.592365959312976, 0.3383759181537529, 0.3306282614128011, 0.326368506022433]","['internet', 'media', 'web']",[],[],[],True,2
2011-02-12-microsoft-buys-nokia-for-0b-what-this-means-for-mobile.md,Microsoft “buys” Nokia for $0B - what this means for mobile.,"['business', 'mobile', 'nokia', 'strategy']","Microsoft “buys” Nokia for $0B - what this means for mobile.
Windows Mobile 7 is the latest incarnation of the Microsoft mobile platform
that started in the late 90s as Windows CE. It has always had relatively good
penetration in the mobile business sector and has been the foundation of HTC
and Samsung's smartphone strategies. For the latter part of the decade though,
Windows Mobile has been languishing, and the release last year of WinMo 7 was
received by the industry with a cursory “I think Microsoft just updated
something to do with mobile” before switching their attention back to what is
happening on the iPhone, Android and Blackberry platforms. To be fair to Nokia, they have been fighting a rearward action for a long time.
Whilst their presence in all markets outside of the US has been and still is
strong, the global landscape is shifting towards the US as a leader in mobile
technology and their position is being slowly eroded. With the launch of the
iPhone and subsequent launch of Android, Nokia and many of the “old”
manufacturers have been finding it tough. Nokia had to partner with someone to make their handsets relevant to the market
again. It transpired last week through conversation coming out of Mobile World
Congress in Barcelona that both Android and WinMo were considered as options
for the new platform. WinMo won out largely because Microsoft has agreed to
spend a lot of money marketing the platform at consumers and developers alike. The latter should not be underestimated in it's impact on a platform. iPhone
and Android have very large shares of mind with developers. Nokia attempted and
failed at doing this in the mid 2000s. Apple and Google have excellent
developer relationship programs (including Google bribing developers with brand
new handsets) and excellent App Store systems. For Apple to have 250,000+ apps
in iTunes there's probably over a million developers who have played with the
development kit and probably similar numbers involved for Android. That's a lot
of “reach out” being done by two companies but between Nokia and Microsoft they
can do this (not least because Microsoft has this function already embedded in
the business). Nokia has said that WinMo phones will be shipping in 12 months – personally I
think they need a handset ready for US Thanksgiving and rest of the world
Christmas as this is a known hot period for sales. Assuming this is the case
who do they target? Well Nokia shifts a lot of phones to people who don't care
what it's running and the fact it's Windows Mobile won't matter at all. Nokia
will continue to sell many tens of millions of handsets to this group just by
virtue of being cheap and able to make calls and send texts. Then there are the “feature phone graduates” - those consumers who will be
looking for their first smartphones. For Nokia and Microsoft these are critical
customers. Give them a positive smartphone experience and they won't shift to
iPhone or some Android device – they won't risk it. Give them a terrible one
and they'll disappear over the horizon when their 2 year contract winds up.
This is the fastest growing group of users and largely responsible for
Android's boom in 2010. However, unless Nokia can produce an extremely slick high end device capable of
competing against an iPhone, Galaxy S or Evo then Nokia will lose the top end
forever. This is very risky ground for Nokia and Microsoft right now. The
reason brands like BMW and Mercedes compete in Formula 1 Racing is to drive
technology that eventually trickles into consumer products but to also create
branding that is aligned with high end technology (which trickles down into
consumer consciousness). Part of Nokia's current woes are that it isn't viewed
as pushing the boundaries like it once was (the Nokia 7710 is the conceptual
grandfather of the iPhone but it's been nearly 10 years since the company was
capable of producing devices like this). The risk for Microsoft in all of this however is that long-term Microsoft
partners, HTC and Samsung may feel alienated by their tight partnership with
Nokia and decide to abandon their Windows Mobile devices completely. WinMo has
already taken a backseat to Android for these manufacturers (and has led to
fantastic growth for HTC and positioned Samsung as now competing against Apple)
so they probably won't be too worried. Microsoft should be though because there
could be 6 month period where there are no Windows Mobile devices launching
into the market. So what does the marketplace look like in 2 years time once Nokia / Microsoft
have had 12 months to shift product? I predict we'll see Android retaining it's number one operating system spot at
somewhere around the 30-35% market share point. After this will come iOS at
approximately 25-30% (especially given Apple's likely iPhone Nano launch at
some point) and then we'll be seeing significant fighting going on between RIM
(Blackberry) and Windows Mobile / Nokia for about 25-30% between them. Symbian
will probably still exist as devices are slowly transitioned to newer hardware. Nokia and Microsoft's first hurdle will be to overtake RIM quickly – within 6-9
months. If the platform doesn't launch with significant momentum (possible
given Nokia's distribution reach) then both companies may as well give up. As a marketer what does this mean? Well you'll still need to market to all of those 100 Million Symbian devices
that were sold last year and the couple of hundred million in circulation that
are connected to the Internet right now. iOS and Android should be your
flagship platforms with degraded support for Symbian, Windows Mobile and
Blackberry. This will give you future proofing as well as picking up the people
actually in the market now without worrying too much about who they might be in
two years time. Of course this flux lends itself to ensuring a flexible mobile strategy – not
going down the path of complex, expensive Native Apps when a mobile website
will be sufficient and more cost effective. The current history playing out in front of us is showing very clearly that
Mobile is the single most important battleground right now and that even
industry Goliath's are not immune to the disruption occurring. Update: dead links removed","['business', 'mobile', 'nokia', 'strategy']","['mobile', 'business', 'strategy', 'nokia', 'android']","[0.6662525649687892, 0.633092731911529, 0.5955361766407105, 0.5840762450026126, 0.32393808371415866]",['android'],[],[],[],True,4
2013-03-20-jump-start-responsive-design-launch.md,Book launch of Jump Start Responsive Design,"['design', 'development', 'responsive design', 'web']","Book launch of Jump Start Responsive Design
Today, ['Jump Start Responsive Design'](http://www.sitepoint.com/store/jump-start-responsive-web-design/)
officially goes on sale. This is the first book I've been an author on and
it was great to work alongside [Craig Sharkie - @twalve](http://github.com/twalve)
on this project over the last 6 months. ![Cover shot of Jump Start responsive Design book](../..//img/posts/responsive_design.jpg)
*[""Jump Start Responsive Design"" - Sharkie & Fisher](http://www.sitepoint.com/store/jump-start-responsive-web-design/)* The book is squarely aimed at developers and designers who are looking to rapidly
get up to speed on current RWD techniques and just get going with them on their
projects. There's plenty of examples that show the current state of where things
are at and how things will also work as browser support increases over time. It was a pleasure to work on this project with Craig as well as the whole team
over at SitePoint. The dead tree version and eBook are available through SitePoint
as well as all the other usual sellers.","['design', 'development', 'responsive design', 'web']","['development', 'web', 'design', 'responsive design', 'mobile']","[0.6586606247498236, 0.6526384954999389, 0.5987078297263396, 0.5973620710956632, 0.3197078446213235]",['mobile'],[],[],[],True,4
2015-06-01-building-portable-minecraft-server.md,Building a portable minecraft server,"['gaming', 'physical computing']","Building a portable minecraft server
My son is pretty into minecraft - just like nearly all kids under the age of 14
at this point in time. He wanted a server that he could use to host a minecraft
world on that he and his mates could play on. Being the security conscious person I am, having some device wide open to anyone
wasn't going to fly. Instead we flipped the interaction to create a mechanism
where they could play and be physically present with each other. As a result of my robotics work there were plenty of Raspberry PIs and LiPo
batteries laying around so it was a fairly straight forward process to get it
all up and running. Minecraft is a lot of fun, especially when you play with friends. Minecraft 
servers are great but they aren’t very portable and rely on a good Internet
connection. What about if you could take your own portable server with you -
say to the park - and it will fit inside a lunchbox? The build process for this is published over at Packt
[Building a portable minecraft server for LAN parties in the park](https://www.packtpub.com/books/content/building-portable-minecraft-server-lan-parties-park) There's also [some code](https://gist.github.com/ajfisher/f61c89733340cd5351a4)
if you just want to get stuck in. ![RaspberryPi running minecraft in a lunchbox with a battery](../../img/posts/minecraft_inbox.jpg)
*Minecraft server in a box - image (CC) ajfisher* If you've got a child who has a group of friends who want to play together, this
is a nice safe way to do it where they get the benefit of collaboration along
with a device that can be more physically secured.","['gaming', 'physical computing']","['physical computing', 'gaming', 'development', 'web', 'hardware']","[0.6198818847741869, 0.589747198813881, 0.34955803074442926, 0.32396551267511736, 0.3116855616090113]","['development', 'hardware', 'web']",[],[],[],True,2
2015-09-02-iot-mobile-practices.md,Applying the lessons of mobile dev to IoT,"['development', 'iot', 'physical computing', 'ubicomp']","Applying the lessons of mobile dev to IoT
I got the opportunity to write an opinion piece for [IBM Developer
Works](http://www.ibm.com/developerworks) on what lessons from other aspects
of technology could be applied to IoT. I drew from my own experience in mobile,
looking at: how decoupling services from avatars create more natural interaction methods assuming connectivity issues and adopting an offline first approach How the hardware & software design process unfolds as a result of interaction
refinement. This challenge is magnified when you consider the numerous contexts in which
your IoT product might be used. It might just be a sensor, but how do you
interact with it? Is it by using a mobile or web application? Is configuration
different than reporting? How usable are these interfaces? The list of
questions can seem endless. Full article at IBM Developer Works: [Best practices for IoT
development](http://www.ibm.com/developerworks/library/iot-mobile-practices-iot-success/)","['development', 'iot', 'physical computing', 'ubicomp']","['development', 'iot', 'physical computing', 'ubicomp', 'web']","[0.6535465056364764, 0.6081999920978352, 0.6047634883969216, 0.6026234070128746, 0.3334490882622506]",['web'],[],[],[],True,4
2023-08-31-ai-works-now.md,AI in 2023 - it works well enough now,"['agents', 'ai', 'conference', 'generative ai']","AI in 2023 - it works well enough now
Recently, I was fortunate enough to get up to Sydney to attend the
[Web Directions AI conference](https://webdirections.com/ai) which provided a
good cross section to see where people are thinking and playing in this space now. I studied AI at Uni in both Computer Science and Psych, but then abandoned
it because one of the (many) [AI Winters](https://en.wikipedia.org/wiki/AI_winter)
set in and I was going to be unemployable. That said, I've maintained a
dedicated interest in the domain, and I have tinkered or worked with many of
the technologies that have led us to where we are today. The day itself was worthwhile – about half of the content I knew and was deeply
familiar with, but it was useful to see others explain it or how they are using
it which I find helps refine my thinking on a topic. The other half was either
new to me or was on an area I have familiarity on but is being used in a
completely new way – this stuff is great to shift the way I think about a given
topic and in particular my ideas about what use cases a particular tech is good for. Many people have asked me about my thoughts since the session and I think these
probably represent a summarised view of the day and my thinking about the area
as of August 2023. No doubt this will change again in a few months given the
pace of change. The excitement and hype are high, but different There have been some wild technology hype-cycles during the
[post-GFC](https://www.rba.gov.au/education/resources/explainers/the-global-financial-crisis.html)
period. Blockchain (and particularly Crypto, NFTs, Web3), Voice Assistants,
as well as AR/VR/XR and the various [Goggles](https://en.wikipedia.org/wiki/Quest_2)
have created an astonishing amount of noise but haven’t really delivered much
in the way of transformation. A lot of this boils down to use cases. In the context of AI, many concrete use
cases have been around for years if not decades, but the AI simply didn’t work.
By contrast, with some of the recent hype-tech we’ve seen interesting bits of
technology that have had to go searching for problems to solve (and are often
trying to solve problems that don’t materially exist) and so they land flat. Numerous speakers at the conference – particularly those who worked in research
pointed out that <b>for a very long time AI technology just didn’t work, but
suddenly, it’s become good enough to do meaningful work</b> and that’s why it’s
grabbing hold in many domains at once. A good example of this is voice and
handwriting detection. I worked on handwriting detection research in the mid-90s
using basic neural networks and it sucked – the training took forever, and the
detection rate was barely passable for someone with extremely clear printed
handwriting. It took another 20 years for that to become good enough to work
for enough people that we don’t even think about it as being cutting edge
technology anymore. Certainly, handwriting detection fails frequently as well
(especially for someone with writing as poor as mine) but for a very large
number of use cases, perfect is the enemy of the good – and good enough can
unlock vast amount of value. The current set of AI technologies are following a similar path as mobile
tech did in the 2000s-2010s. Rather than building on and enhancing what came
before it incrementally, the enhancement provided is by an order of magnitude
or greater. For example, the shift to mobile meant that computing was no longer
""something you sit at a desk"" to undertake. Instead, computing became permeated
into every aspect of our lives (for good and bad) and was as fundamental a
change as moving off mainframes to PCs. ![Mobile unblocked tremendous computing opportunities](../../img/posts/freestocks-hRVrvH9-dG0-unsplash.jpg)
*[Mobile untethered computing from desks. Photo by freestocks on
Unsplash](https://unsplash.com/photos/hRVrvH9-dG0)* By following a similar trajectory, AI is driving excitement as a result (in a
similar fashion as the move to mobile or the advent of the web for those who
bore witness to those periods). Even current AI tooling is getting good enough
to do new things, better deliver old uses or create new opportunities and
people are right to be happy about that (especially considering it’s taken
nearly 70 years of research and applied development to get to this point). The interaction model is undefined No one really has a good sense of what the AI interaction model looks like.
As humans our default interaction models are to use a tool directly (eg excel)
or to talk to a person (eg speak to a friendly data analyst). When we use tools,
in most cases a good tool (physical or virtual) gets out of the way and becomes
an extension of the wielder’s brain and hand to achieve the intended goal.
Likewise, if we are asking someone else to do some work, we say what we’d like,
have a conversation back and forth to clarify, then let them get on with using
the tools because they have expertise. AI ""tools"" as they exist right now, sit somewhere between these two
interaction models. Think of something like [ChatGPT](https://chat.openai.com/) / 
[Bard](https://bard.google.com/) or [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release)
/ [Midjourney](https://www.midjourney.com/) and you’ve got to talk (chat) to
your tool to make it work. It doesn’t feel quite right to use. My sense is that in many cases this is why people initially try to get the
tools to do something silly (eg ""explain how a nuclear reactor works as
lyrics in a rap battle"") - because they haven’t got a strong mental model for
how this works. People clearly know they aren’t talking to a human so they
feel they need to give it instructions that they could never get a human
they know to follow. ![ChatGPT explains a reactor in the style of a rap](../../img/posts/reactor_rap_battle.png)
*ChatGPT explaining nuclear reactors, image (cc) ajfisher* The upside of this is that this means there are many opportunities for
developing what future mental models we’ll all use might look like. As a
result, this invites the opportunity for play, innovation, and research while
there’s no rules set in stone (or at least ""best practices""). The point of caution here though is that users have got a good radar for 
this already because it is so new. Because of this, it means that just
slapping ChatGPT onto your existing product will feel weird and just ""tacked on""
and customers won’t see the benefit especially if it’s in a human support role
where it can end up being dehumanising. How and where you build is up for grabs Risk was a huge topic. Many of the speakers touched on this as well as a large
number of people I talked to. With only a couple of viable players in town
(who are global) and with extremely limited Open Source options (LLAMA 2 sort
of aside) there was a lot of consideration being given to what happens if someone
goes bankrupt or faces significant legal or regulatory action (eg some of the
copyright considerations currently being levelled at OpenAI). If your business is ""all in"" on one provider to drive your product capability
then your business will be severely impacted. A lot of organisations, in their
rush to get something built, aren’t acknowledging the commercial risk they are
carrying as a result. The outtake here is that some clear-eyed members of the
business need to make their voices heard on this front so organisations are
taking a more robust approach. Aligned to this is where these models are running is important as well.
There’s wide acknowledgement that whilst having access to ChatGPT or BingChat
or Bard or whatever is quite cool, there’s inherently a stack of latency
involved which again makes those interaction models clunky. They also use a
stack of bandwidth if you’re thinking about low powered or poorly connected
devices out in the world that may want to consume these services. As a result everyone is looking to push AI to the edge. The next game in town
is really about being able to <b>push workable versions of those models down to
devices so they can run locally</b> (eg in browser) by optimising their contexts
and then remove 100% of the network latency which will make them way more
interactive and potentially offline capable. Recent advances with [LLAMA 2](https://ai.meta.com/llama/) and the ability to
run it on many targets
([including on a raspberry Pi](https://www.tomshardware.com/how-to/create-ai-chatbot-server-on-raspberry-pi) -
though infuriatingly slowly) highlight the desire to optimise for pragmatic,
""good enough"" models that can be run close to where the need is. Potentially by
sacrificing generalised models with wide capabilities for smaller, more focussed
ones we will see an upswing in the amount of AI at the edge in coming months. Everyone needs a co-pilot As part of the conference, we got to see 2 minute pitches from 20 businesses
in Sydney that were building products with AI. At one level it was great to
see a thriving start-up ecosystem in place in an Australian city but it
highlighted that a lot of people were building on top of a couple of AI APIs
and calling that a product (see many of the points above about the dangers of this). What was very evident from the pitches, as well as numerous examples given
by the speakers, is that there’s a huge amount of activity happening around co-pilots. This seems like it’s being driven by the unlock of
[LLMs](https://en.wikipedia.org/wiki/Large_language_model) - which have got quite
good in a general sense and can be fine-tuned with domain specific information
to focus their attention (and minimise hallucination). This is really speaking
to commercial models that are considering ""cost savings"" as a motivator though
it’s less about cutting jobs and more about being able to scale the ones you
have by making them more effective or eliminating less valuable effort for a
given individual. ![Github co-pilot is the model](../../img/posts/ai_copilot.png) The key with a lot of these products is going to be their interaction model and
how quickly you can get them to do the work. If I have to spend 30 minutes
crafting a prompt to save myself 40 minutes of work, arguably I could probably
just do the work and keep my brain on-task. Obviously we’ve seen [GitHub’s co-pilot](https://github.com/features/copilot)
in action for a while now and they seem
to have done a lot of good work thinking about how this tool sits alongside
the existing developer experience (if you use VS Code at least). How this
manifests in other domains and isn’t just an enhanced Clippy remains to be seen. There’s a lot more to come We’re seeing things now that have been struggling to get out of labs for over
20 years and they are just now good enough to do so. In achieving these
successes – particularly regarding GPU training and model execution on
devices – this has generated a lot of learning that will now be applied to
other domains. I don’t envisage this being a case of every problem will be solved by an LLM
or Stable Diffusion approach, but <b>the practical learning that has arisen from
these techniques can be taken and refine other tools</b> that we’ll be able to put
in our collective arsenal to solve an ever-widening set of problems. One area I haven’t really touched on in this post is the issue of ethics and
equity as it relates to AI. This was a topic that kept being touched on, however
it is evident that the need for more conversations about these aspects is
crucial if we want this technology to be used in ways that eliminate or
minimise harm. The technology is racing ahead however, regardless of the
ethical or equity considerations which for me is alarming. Overall, I’m glad I was able to get up to Sydney to attend the conference as
it was good to see a lot of thinking in one place and at a moment in time to
see where this all heads next.","['agents', 'ai', 'conference', 'generative ai']","['ai', 'generative ai', 'conference', 'agents', 'development']","[0.6162365519420198, 0.5872881491621578, 0.581613351738307, 0.5749584488866017, 0.3243349932175174]",['development'],[],[],[],True,4
2025-02-03-retail-improvements-australia.md,After the drought: first shoots emerge for Australian retail,"['business', 'ecommerce', 'growth', 'retail']","After the drought: first shoots emerge for Australian retail
Anyone working in retail in Australia (or globally) will tell you it has been a
very hard slog. The combination of slow growth into the pandemic, followed by
[supply chain
issues](https://www.ey.com/en_au/insights/supply-chain/how-covid-19-impacted-supply-chains-and-what-comes-next),
[interest rate hikes](https://tradingeconomics.com/australia/interest-rate)
then rapid inflation has meant discretionary retail has been in troubled waters
for a little while now. This week, [the ABS released its December 2024 retail spending
report](https://www.abs.gov.au/media-centre/media-releases/retail-spending-steady-december),
and while I doubt CFOs will be unlocking their budgets just yet, it does
show that there’s the chance we’ve turned the corner in Australia. It’s not all unbridled optimism though. If a large-scale trade war erupts -
especially due to US tariff policies and China’s reaction - the modest growth
we’re seeing here could easily wither over the coming quarters. Signs of growth - if you know where to look Overall, retail had the strongest December quarter it has had in years. At
best, accounting for inflation, it’s been flat - and the reality is it’s
probably been down once you take that into account. While December specifically was down month-on-month by a very slim 0.1%, this
was off the back of strong gains in October (+0.5%) and November (+0.7%). Consumers are beginning to feel more confident about the economy and a lot of
this is them just knowing where they stand financially. If your rent or
mortgage keeps going up every other month and the prices you pay at the
checkout keep changing, it is incredibly hard to plan your expenditure. As a
result, people stopped spending. To an extent, this was exactly the RBA’s plan - to cool the economy and curb
inflation. I won’t pass judgement on how they did that, other than to say for
the last 24 months, consumer spending has been low and discretionary spending
has been severely curtailed. For retail, this has had huge implications with many stores closing and has
been particularly hard on [restaurants and cafes who have seen record
failures](https://www.news.com.au/lifestyle/food/restaurants-bars/there-lies-the-problem-cafes-restaurants-failing-at-fastest-rate-on-record/news-story/d2267f48e9692f8d870cae9ba541d534). These latest figures indicate that consumers have finally got a read on their
finances. Inflation is back within target band for most items, and mortgage and
rental rates have largely stabilised (even if they are costing more of a
fraction of income than any time before). Additionally, many consumers have had
a pay rise over the last 18 months. Most consumers aren’t exactly flush with cash, but the economic landscape is
much more stable than before, fostering confidence and a slight loosening of
the purse strings. This means some additional discretionary spend. The data shows household goods and department store sales are up while clothing
and cafes are down. This makes sense - if you are going out less then you are likely at home more,
so you want your home space to be a bit more comfortable. A knock on
implication of this is that you need fewer clothes. We know the op-shops are
[doing very good trade
over](https://www.abc.net.au/news/2024-05-08/australians-turning-to-op-shops-in-cost-of-living-crisis/103809766)
the last 12 months as a result of people not buying new. We also know that
[Temu and Shein are proving
unstoppable](https://www.roymorgan.com/findings/9646-shein-and-temu-contintue-to-grow-strongly-august-2024)
for cheap fashion and homewares - none of which is really captured in the ABS
numbers for retail spending. Retail spending increased +0.5% per capita over the quarter which is the first
uptick in more than 2 years - a good sign of sustainable consumer confidence. Pulling sales forward Sales are moving from December and into November (or even earlier) - a trend
that’s been building for nearly a decade and is becoming more prominent than
ever. The impact of BFCM, Singles Day and Click Frenzy is pushing November
sales higher, away from December being the retail peak. Working with all my retail clients coming into the end of the year, this
behaviour was easily apparent and is very much in line with the long term
trend. This has implications for supply chains, seasonal drops and profitability over
the quarter and I’m not sure any retailers have fully worked through all the
implications of this now just being how consumers behave. In 2025 and 2026 I
believe we’ll see less blanket discounting with better targeting but also more
selectivity when it comes seasonal and new products than we’ve so far. Where to from here? Consumers have told the retail sector that they are prepared to spend again -
but you have to have a good product at a good price. Many buyers are being
extremely picky about their spending and are quite happy to wait until a sale
comes around so they can get a discount and keep hold of their hard-won
discretionary dollars. There is a great deal of variance across localities, verticals and audiences so
understanding exposure to this is imperative for retail leaders to manage risk
but also understand where the opportunities fall. Targeting of audiences from
both an acquisition and retention standpoint is going to be key here -
especially if you can frame additional benefit beyond price parity. Unlocking
initiatives that help enable this would be good investments of human and
financial capital. The last few years have been some of the most difficult I’ve seen in retail for
my entire career - reflective of it being a difficult period of time for
consumers. That said, the numbers for the December quarter are the most
positive they have been across the retail sector since the start of the
pandemic - multiple indicators suggest that while we aren’t out of retail
winter just yet, we might be entering a thaw and there are a few patches of new
growth that indicate spring may be on the way. Notes: _This post originally started as a [Bluesky
thread](https://bsky.app/profile/did:plc:gaf7g3frhn47hljbcipvidxj/post/3lhase3rwx22f)_","['business', 'ecommerce', 'growth', 'retail']","['business', 'retail', 'growth', 'ecommerce', 'mobile']","[0.6287373888154738, 0.5965406315336012, 0.5944312165297295, 0.5853589111476867, 0.3117295190903512]",['mobile'],[],[],[],True,4
2025-06-26-hands-on-gemini-cli.md,Hands on with Gemini CLI,"['agents', 'ai', 'development', 'generative ai']","Hands on with Gemini CLI
It seems like the CLI-based agents are coming in thick and fast now with
[Google finally catching
up](https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/)
to [Claude Code](https://docs.anthropic.com/en/docs/claude-code/overview) from
Anthropic and [Codex CLI](https://github.com/openai/codex) from OpenAI. And
that’s not to mention the adjacent world that lives in the IDE courtesy of
[GitHub](https://github.blog/news-insights/product-news/github-copilot-meet-the-new-coding-agent/),
[Cursor](https://www.cursor.com/), [Windsurf](https://windsurf.com/) and
[others](https://ampcode.com/). Since [I/O in May](https://io.google/2025/), Google has been emitting a steady
stream of PR releases on various AI tools and products they are making
available. The cynic in me feels this is all about Google trying to convince
the world that they are important and relevant after having been thoroughly
outpaced in the opening year or two of AI products coming to market. [AI
research at Google](https://ai.google/research/) does proceed at pace and
continues to make significant headway - but that doesn’t win  consumer
eyeballs. Today, Google released [Gemini
CLI](https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/),
their command line based tooling primarily aimed at developers who want to not
just produce code, but also run commands related to it or orchestrate other
parts of their system. For me, very much a CLI-oriented developer, this feels much more aligned to my
way of working so I continue to play with these tools to see how they perform. One of my gripes with Codex CLI and Claude Code is you’re paying per token, and
trying to evaluate how many tokens you’ll need to do a task (especially with
chatty reasoning models) is almost impossible to determine ahead of time. So
it’s very unclear whether a task might cost $0.30 (very, very unlikely), $3.00
(relatively frequent) or $30.00 (not as infrequent as I’d like). In the Preview at least, Gemini CLI is more flexible, with high default usage
limits which recycle every day, and if you exceed the limit, you get dropped
down from Gemini Pro 2.5 to Gemini Flash 2.5 - a less capable but still very
good model (though this behaviour is undocumented). First tasks Set up was very [straightforward](https://github.com/google-gemini/gemini-cli). Dropping into the Gemini command line, it looks and behaves very much like
Claude Code and Codex CLI and so I started with the usual task of getting the
tool to give an overview of the application and see what it produced. ![Start up and ask some questions](../../img/posts/gemini-cli-start.png) Nothing about this task is too complex but it distills the key elements pretty
well. ![Description of the set up for this
blog](../../img/posts/gemini-cli-description.png) I put it through my usual routine, “explain this particular file” (which I know
is complicated), “look at the front end and suggest 3-4 high value
improvements”, “refactor this component to be more testable”. Against all of
this it performed exactly as I’d expect it to - it didn’t make fewer errors
than any other model, nor did it make more. Ultimately it’s on par with
everyone else. Except, we have that monster [1M token context window in
Gemini](https://ai.google.dev/gemini-api/docs/long-context). And, I’m not paying by the token during the preview. A bigger ask Let’s see if it can do something I’ve been putting off for months - modernising
an old front end application I built years ago that is very creaky. So I gave it a brief to build a new front end project in my repo; move to vite
with react and typescript, get rid of Styled Components
([RIP](https://x.com/mxstbr/status/1908201327811059926)) and refactor using CSS
modules and also remove react-redux. The plan it came up with was solid: ![Start up and ask some questions](../../img/posts/gemini-cli-plan.png) I let it start with Phase 1, which it completed without any dramas and, inside
a few mins, had everything in the right place without too much trouble. I then moved it on to Phase 2 - which proceeded remarkably smoothly… until it
didn’t. The first warning I got was seeing a 429 error (too many requests) which I
thought was weird as it wasn’t going that fast and google claimed it could go
at 60 requests per minute. Certainly visibly it wasn’t going that fast. Pausing for a minute then continuing on I then got rate limited. Apparently, in
about 10 minutes I had exhausted the daily 1K request limit for Gemini Pro
which isn’t visible in the CLI tool itself. ![Rate limited](../../img/posts/gemini-cli-rate-limit.png) Unfortunately, in the transition to Gemini Flash instead which you can continue
to use, the process got stuck in a loop that persisted and there was no way to
exit. The gemini CLI team are aware of these issues with a number of github issues
raised and they seem to be actively patching pretty quickly. So I expect these
little hiccups will resolve. That said, the bit of refactoring work I got it to do before falling over was
genuinely solid. It established a structured approach to convert components and
it proceeded methodically, it just burnt a tonne of requests doing it and then
didn’t quite have the methods to handle that failure. Several months ago even I’d have never even attempted something this complex as
a brief - even just the component refactor is over 100 individual components
needing changes and management. So the baseline expectation of what these tools
can do is still moving upwards rapidly. Final thoughts on Gemini CLI My core grumble with these tools, is that it’s almost impossible to understand
how many tokens / requests are going to get burnt executing the task and what
the recovery mode is when it does. In many cases the people using these tools
are either getting free tokens (ie staff) or have no budget constraints (ie
burning someone else’s money) - visibility over this side is critical to drive
better adoption and cost prediction. What I’d love to see from Gemini CLI is the ability to simply throttle all the
way back and then pick up the task when the request limit refreshes. Better
yet, being able to gracefully suspend then restore to run at a later point from
where it left off would be great. Will I keep using Gemini CLI? Yes, definitely for smaller, self-contained under
my orchestration. Even with the limits, the time saved makes it worthwhile. A weekend project might be to find another problem to throw at that wild
1M-token context window and see how it goes.","['agents', 'ai', 'development', 'generative ai']","['development', 'ai', 'generative ai', 'agents', 'web']","[0.6343463137904906, 0.6115809044751209, 0.5917190522668905, 0.5871001733323548, 0.326866125627965]",['web'],[],[],[],True,4
2025-07-02-qantas-breach-toxic-waste.md,The Qantas breach proves personal data is toxic waste,"['business', 'privacy', 'security']","The Qantas breach proves personal data is toxic waste
Data breaches come with such regularity now, that getting annoyed about any
one in particular feels nonsensical. That said, while the frequency may be
increasing, we can't accept this becoming normal. We’ve hit the point where essentially no personal information given to a
business can be considered secure, and that it’s only a matter of time before a
breach occurs. This doesn't mean surrendering and letting bad actors run
roughshod, it means managing this risk as though this is the case. It wasn’t really a surprise then, that this week Australia’s flagship airline,
Qantas, [announced that they had a data
breach](https://www.qantas.com/au/en/support/information-for-customers-on-cyber-incident.html)
that affected 6M customers. In the grand scheme of the hundreds of millions of
people affected in recent breaches, this barely ranks at all. Even in Australian
terms it’s not an [Optus](https://en.wikipedia.org/wiki/2022_Optus_data_breach)
or a [Medibank](https://en.wikipedia.org/wiki/Medibank#2022_cyberattack) in
terms of scale. Exhibiting a bit of gallows humour, as I was about 99% confident my details
would be in the data, I took to
[BlueSky](https://bsky.app/profile/ajfisher.social/post/3lswvoeq7o22w) and
[LinkedIn](https://www.linkedin.com/posts/andrewfisher_privacy-qantas-security-activity-7345975837722034178-_IzI/)
to note that Qantas would now work through the “Data Breach PR Playbook”: Acknowledge the incident (only because of the [mandatory breach disclosure
laws from 2018](https://www.oaic.gov.au/privacy/notifiable-data-breaches)) Downplay the impact of the data loss (no credit card, passport info of
passwords leaked) Send out some comms to affected customers that reiterates the above but adds
""be on the alert for scams that look like they are from us"". (What I called
""hearts and minds"" comms). The dismissiveness regarding personal information that accompanies this
playbook irritates me no end. In 2025, changing a password or blocking and reissuing a Credit Card are so
trivial as to almost be a non-event for most people. However, information like my name and date of birth are effectively immutable
and, in a highly digitised economy like Australia, being forced to change my
phone number or email address is going to affect everything from my utility
bills, to notifications from my kid's school to contact details with my doctor.
Changing them is possible, but it’s practically very difficult and time
consuming - with big implications when I do. ![Email from Qantas about the breach](../../img/posts/qantas_breach_letter.png)
*Screenshot of email sent from Qantas about data breach* When it came time to issuing their statement to affected customers a few hours
later, Qantas even doubled down on this message, reiterating that no card
details or passwords had been stolen. They mentioned the other points that had
been lost in passing (and only really because the law forces you to have to say
what was disclosed). Proliferating fraud potential Yes, credit card numbers and identity theft are real problems, and cost the
community a huge amount annually. But in terms of fraud, this is becoming small
beer. [Targeted, sophisticated, personalised
scams](https://en.wikipedia.org/wiki/Pig_butchering_scam) are now estimated to
be worth hundreds of billions (yes, with a B) of dollars a year and growing. And
that could be underestimated. These scams don’t require credit card numbers or identity - what they
require is trust and connection. With a data breach of this type, information like date of birth
allows scammers to do segmentation and potentially target messaging
at particular cohorts in different ways to maximise effect (eg a professional
connection vs a potential love interest). ![Illustration of a scammer building up a target](../../img/posts/scammer.png)
*The more data for personalisation the better for scammers. (cc) ajfisher - Flux.Dev* This data, along with that from other breaches, adds to the potential detail a
scammer can draw on to craft a plan to make contact with, then swindle their
intended victim. Downplaying the importance of these details being breached is an absolution of
responsibility on behalf of the company who held that data in the first place.
However, it also doesn’t alert the customer to the potential implications of
this data being leaked, and disempowers them in the ongoing fight against scams. Not withstanding more sophisticated scams, the ability of fraudsters to build a
replica of the Qantas rewards store to capture login credentials is virtually
trivial. These could then be used to login as a customer, redeem their points
against gift cards and then be launder them for cash - a potentially
lucrative pay day with almost zero recourse. Personal data is toxic waste The teams I work with get sick of me talking about this. We might need to
capture personal information to facilitate outcomes via digital platforms (eg
you will need to fill in your personal info if you want something shipped from
an online store) - but only capture what is necessary and only send it as far
as is required. Yes, there are tradeoffs inherent in this decision but in many organisations
the desire for more data, ""just in case"", is a powerful force driving that
discussion. ![Data is a form of toxic byproduct](../../img/posts/data_waste.png)
*Consider long term PII as a toxic byproduct of our systems* Qantas clearly care about the security of personal information so much that
they let a third party have access (noted by them) to personal details of 6
million customers. I suspect the reason for this is entirely mundane, and is something like
managing an outsourced contact centre to deal with customer support. This is
speculation on my part, but is not uncommon for many enterprises in my experience. Given 6 million records is probably the full active base of customers Qantas
have, why would a third party need access to all of that information? Why are
the whole customer records being synced? Even assuming it’s a customer contact automation platform of some type (eg
Salesforce), why would there be a level of access that could cause that type of
breach? And why that level of personal information stored in it (such as dates
of birth)? These will be some of the questions that need to be answered as part of this
process - though I suspect they won't be tackled head on because orgs only care
about ""scary"" details like passport numbers, credit card details and passwords
due to customer perception that this is ""serious"". Personal data storage needs a rethink If you design systems assuming that eventually they may be breached, your
design decisions fundamentally change. This assumption starts a different line of questioning regarding what happens
if a breach occurs in a system. Organisations need to completely rethink their privacy and data storage
practices and orient towards an assumption of disclosure and the implications
of that, rather than the mistaken belief they can protect all data in all
systems all the time. This is provably incorrect. If you adopt this mindset then the design of your system changes: The best security of personal information is not capturing it in the first
place. Do you actually need this data and if so, what for? Expire and then purge data when it’s no longer needed (compliance and
regulatory requirements may drive the timing of this). If the process
requiring the data is finished, why hang on to it? Tokenise sensitive data, refer to it and then use it “just in time” to do the
action you require. Why do we need someone’s email address in every system?
Why not hold a reference to an encrypted version instead right up to the
point of sending an email. Privacy by design takes more effort up front and requires architectural
discipline to achieve in a meaningful way across the enterprise. Likewise, many
vendors need to support things like just in time token exchange in their
systems. But, just because something is hard, doesn’t mean it isn’t worthwhile. It is becoming increasingly rare for major data breaches of credit card
information to occur. This is a result of legislation and fines, ensuring that
businesses are taking this seriously as well as technology systems being
improved to no longer require storage (such as card tokenisation for repeat or
recurring purchases). The <b>improvements made relating to card security has been a sustained and
collaborative effort by all players</b> in the system. This came by recognising
that there was a problem that needed action, and then working through the detail
over long periods of time to drive the outcome. None of this stuff is easy, but as we have seen this week with Qantas and the
dismissiveness that still occurs around the disclosure of personal information,
we're still some way away from getting everyone to agree that there’s a problem
that needs solving here. I'm hopeful we'll get there eventually, and also hopeful that governments
legislate harder to create incentives for organisations to design
appropriately. In the same way that environmental and waste laws drive
organisational behaviour, if we start treating personal data as a toxic
byproduct of digital services then legislation to protect from these byproducts
makes sense. Legislation like the
[GDPR](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation)
(with the teeth that come from fines) and the
[CCPA](https://en.wikipedia.org/wiki/California_Consumer_Privacy_Act) are good
examples of a starting point but need to go wider, deeper and have tough fines
so the calculus of deterrence makes sense for companies to take action.","['business', 'privacy', 'security']","['business', 'security', 'privacy', 'web', 'rant']","[0.6230435447258529, 0.5995996814765618, 0.5867791392071053, 0.3226728371061907, 0.3087578491256337]","['rant', 'web']",[],[],[],True,3
